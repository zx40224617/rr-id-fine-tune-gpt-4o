{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The post-pandemic hospital and mortality burden of COVID-19 compared with influenza: A national cohort study in Denmark, May 2022 to June 2024\nAbstract: BackgroundIn the post-pandemic period, COVID-19 continues to cause significant numbers of hospitalisations and deaths. We describe this burden and compare it to the burden of influenza in the first two post-pandemic years in Denmark.  MethodsA cohort study including residents in Denmark from May 16, 2022, to June 7, 2024. Data were obtained from national registries, including information on Polymerase chain reaction (PCR) test-positive COVID-19 and influenza admissions, mortality within 30 days of admission, sex, age, COVID-19 and influenza vaccination, comorbidity, and living in long-term care facility for elderly. Negative binomial regression was used to estimate adjusted incidence rate ratios (aIRRs) to compare rates of admissions between COVID-19 and influenza. To assess severity of COVID-19 among hospitalized patients, we used Cox proportional hazard models to estimate adjusted hazard ratios (aHR) of 30-day mortality between COVID-19 and influenza.  ResultsAmong 5,899,170 individuals, admissions with COVID-19 (n=24,687) were more frequent than admissions with influenza (n=8,682; aIRR 2.01, 95%CI 1.37-2.95), in particular during the first year (p=0.01), in the summer (p<0.001) and among people above 65 years of age (p<0.001). The number of deaths were also higher (COVID-19, n=2,393; influenza, n=522). Among patients, the risk of mortality of COVID-19 was higher than influenza in the 12-30 days following admission (0-11 days, aHR 1.08, 95%CI 0.94-1.25; 12-30 days, aHR 1.50, 95%CI 1.21-1.84), in particular among non-vaccinated for both COVID-19 and influenza (aHR 1.81, 95%CI 1.25-2.62), while it was similar to influenza among patients without comorbidities (aHR 1.07, 95%CI 0.63-1.80).  ConclusionCOVID-19 represented a greater disease burden than influenza, with more hospitalisations and deaths, and more severe disease primarily among non-vaccinated and comorbid patients. These results highlight the continued need for attention and public health efforts to mitigate the impact of SARS-CoV-2.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Suicide Death Predictive Models using Electronic Health Record Data\nAbstract: In the realm of medical research, particularly in the study of suicide risk assessment, the integration of machine learning techniques with traditional statistics methods has become increasingly prevalent. This paper used data from the UNC EHR system from 2006 to 2020 to build models to predict suicide-related death. The dataset, with 1021 cases and 10185 controls consisted of demographic variables and short-term informa-tion, on the subjects prior diagnosis and healthcare utilization. We examined the efficacy of the super learner ensemble method in predicting suicide-related death lever-aging its capability to combine multiple predictive algorithms without the necessity of pre-selecting a single model. The study compared the performance of the super learner against five base models, demonstrating its superiority in terms of cross-validated neg-ative log-likelihood scores. The super learner improved upon the best algorithm by 60% and the worst algorithm by 97.5%. We also compared the cross-validated AUCs of the models optimized to have the best AUC to highlight the importance of the choice of risk function. The results highlight the potential of the super learner in complex predictive tasks in medical research, although considerations of computational expense and model complexity must be carefully managed.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Associations of Smoking Status and Leisure-time Physical Activity with Waist Circumference Change Ten-year Follow-up among Twin Adults\nAbstract: BACKGROUNDThis follow-up study investigated the associations of smoking status and leisure-time physical activity (LTPA) with weight circumference (WC) change.  METHODSIn the FinnTwin16 cohort, 3,431 twins (47% men) reported smoking status, LTPA, and WC in early adulthood and 10 years later. Regression models were conducted to investigate associations of smoking status and of LTPA change (metabolic equivalent tasks [MET]-h/week) with WC change (cm) during the follow-up. Within-pair associations were analyzed using linear mixed fixed-effect regression among 800 same-sexed (409 identical) pairs.  RESULTSDuring the 10-year follow-up, 40% (n=454) quit smoking. Among those who quit smoking, the mean WC increase was 7.4 cm (SD 8.2) and the mean LTPA decrease was -0.02 MET-h/week (SD 35.8). Compared to individuals continuing daily smoking, only quitters who smoked daily at baseline ({beta} 1.87; 95% CI 0.68, 3.06) increased their WC. This association was not robust after shared familial influences were controlled for. Each additional MET-h/week lowered the risk for WC increase among individuals who smoked occasionally ({beta} -0.054; -0,08, -0.003), quitters who smoked daily ({beta} -0.05; -0.06, -0.02) and those who had never smoked ({beta} -0.04; -0.05, -0.03). In the analyses among identical twin pairs, LTPA was associated with less WC increase among those who quit occasional smoking or had never smoked. For quitters from daily smoking, this association approached significance, but no association remained for those continuing smoking.  CONCLUSIONSSmoking cessation seems to be associated with WC increase, but familial confounding is involved in this process. LTPA may inhibit post-cessation WC increase.  KEY MESSAGESThis will be published as a summary box after the abstract in the final published article.  What is already known on this topic1. Individuals quitting smoking tend to gain weight, but less is known of post-cessation abdominal obesity and the role of leisure-time physical activity in this process.   What this study adds2. Smoking cessation associates with moderate increase in waist circumference among those who quit daily smoking, but shared familial influences are involved in this process. 3. Increase in leisure-time physical activity may prevent post-cessation waist circumference increase.   How this study might affect research, practice, or policy4. More research is needed to unravel genetic and epigenetic mechanisms underlying post-cessation waist circumference increase. 5. In practice, individuals who quit smoking should be encouraged to be physically active.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Detection of diphtheria toxin-producing Corynebacterium ramonii in wounds of an urban, inner-city population in Vancouver, Canada, 2019-2023\nAbstract: We conducted a retrospective chart review and whole-genome sequencing of specimens with presumptive Corynebacterium ulcerans in Vancouver from July 2019 to July 2023. WGS confirmed 8/14 wound culture isolates as C. ramonii. Two distinct clusters of C. ramonii were identified with clinical presentations similar to cutaneous C. diphtheriae.  Article SummaryTwo distinct clusters of C. ramonii were identified in Vancouver, BC with clinical presentations similar to cutaneous C. diphtheriae.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Synergizing Health Strategies: Exploring the Interplay of Treatment and Vaccination in an Age-Structured Malaria Model\nAbstract: Malaria remains a persistent global challenge, particularly prevalent in tropical regions of Africa, Asia, and South America. According to the World Health Organizations (WHO) World Malaria Report 2023, there were approximately 249 million reported malaria cases in 2022 across 85 endemic countries, resulting in over half a million deaths. Progress towards global malaria eradication through antimalarial drugs has been slow, with case numbers increasing since 2015. Dihydroartemisinin (DHA), artesunate, and artemether, derivatives of artemisinin, are crucial components of modern antimalarial treatment. However, resistance to these drugs and their partners in Artemisinin Combination Therapy (ACT) has emerged in Southeast Asia, Africa, and South America. In 2021, the WHO recommended widespread use of the RTS,S malaria vaccine among children in endemic regions. To address these challenges, we develop an extended SEIR age-structured model incorporating malaria vaccination for children, drug-sensitive and drug-resistant strains, and interactions between human hosts and mosquitoes. Our research focuses on evaluating how malaria vaccination coverage influences disease prevalence and transmission dynamics. We derive the basic, intervention, and invasion reproduction numbers for both strains and conduct sensitivity analysis to identify key parameters affecting infection prevalence. Our findings reveal that model outcomes are primarily influenced by scale factors that reduce transmission and natural recovery rates for the resistant strain, as well as by drug treatment and vaccination efficacies, and mosquito death rates. Numerical simulations indicate that while treatment reduces the malaria disease burden, it also increases the proportion of drug-resistant cases. Conversely, higher vaccination efficacy correlates with lower infection cases for both strains. These results suggest that a synergistic approach, involving both vaccination and treatment, could effectively decrease the overall proportion of the population that is infected.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Epidemiological Analysis of Confirmed Mpox Cases in Burundi, July to September 2024\nAbstract: This study analyzes mpox cases in Burundi from July to September 2024, following the 34 introduction of Clade Ib. During this time, 25.4% of 607 samples tested positive via PCR. 35 Children under 15 comprised 55.2% of cases, with a higher proportion of female children testing 36 positive. Geographic analysis demonstrates case concentration in Bujumbura Mairie (59.1%). 37 These findings highlight the importance of age- and sex-specific interventions for outbreak 38 containment and the need for targeted public health strategies in Burundi.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Predictive Time Series Modeling of Malaria Incidence in India: A 20-Year Retrospective and 5-Year Forecast.\nAbstract: This study presents a predictive time series analysis of malaria incidence in India, utilizing a two-decade dataset. Malaria, caused by Plasmodium species and transmitted by Anopheles mosquitoes, poses a significant public health challenge in the WHO South-East Asia Region. We employed linear regression and ARIMA modeling techniques to analyze historical incidence data, ensuring rigorous data cleaning for accuracy. Results indicate a statistically significant declining trend in malaria cases, with projections suggesting continued decreases in malaria cases over the next five years. These findings highlight the effectiveness of current malaria elimination strategies in India and underscore the necessity for sustained public health interventions. Ultimately, this study emphasizes the importance of data-driven approaches for understanding and forecasting malaria trends, contributing to the goal of malaria eradication by 2030.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The transmission blocking activity of artemisinin-combination, non-artemisinin, and 8-aminoquinoline antimalarial therapies: a pooled analysis of individual participant data.\nAbstract: Background Interrupting human-to-mosquito transmission is important for malaria elimination strategies as it can reduce infection burden in communities and slow the spread of drug resistance. Antimalarial medications differ in their efficacy in clearing the transmission stages of Plasmodium falciparum (gametocytes) and in preventing mosquito infection. Here we present a combined analysis of six trials conducted at the same study site with highly consistent methodologies that allows for a direct comparison of the gametocytocidal and transmission-blocking activities of fifteen different antimalarial regimens or dosing schedules. Methods and findings Between January 2013 and January 2023, six clinical trials with transmission endpoints were conducted at the Clinical Research Centre of the Malaria Research and Training Centre of the University of Bamako in Mali. These trials tested Artemisinin-Combination Therapies (ACTs), non-ACT regimens and combinations with 8-aminoquinolines. Participants were males and non-pregnant females, between 5-50 years of age, who presented with P. falciparum mono-infection and gametocyte carriage by microscopy. Blood samples were taken before and after treatment for thick film microscopy, infectivity assessments by mosquito feeding assays and molecular quantification of gametocytes. Mixed-effects generalized linear models were fit with individual-specific random effects and fixed effects for time points, treatment groups and their interaction. Models quantified changes in mosquito infection rates and gametocyte densities within treatment arms over time and between treatments. In a pooled analysis of 422 participants, we observed substantial differences between ACTs in gametocytocidal and transmission-blocking activities, with artemether-lumefantrine (AL) being significantly more potent at reducing mosquito infection rates within 48 hours than dihydroartemisinin-piperaquine (DHA-PPQ), artesunate-amodiaquine (AS-AQ) and pyronaridine-artesunate (PY-AS) (p<0.0001). The addition of single low dose primaquine (SLD PQ) accelerated gametocyte clearance and led to a significantly greater reduction in mosquito infection rate within 48-hours of treatment for each ACT, while an SLD of the 8-aminoaquinoline tafenoquine (TQ) showed a delayed but effective response compared to SLD primaquine. Finally, our findings confirmed considerably higher post-treatment transmission after sulfadoxine-pyrimethamine plus amodiaquine (SP-AQ) compared to most ACTs, with a significantly lower relative reduction in mosquito infection rate at day 7 compared to DHA-PPQ, AS-AQ, and AL (p<0.0001). Therefore, adding an SLD PQ to SP-AQ may be beneficial to block malaria transmission in community treatment campaigns. Conclusions We found marked differences among ACTs and single low-dose 8-aminoquinoline drugs in their ability and speed to block transmission. The findings from this analysis can support treatment policy decisions for malaria elimination and be integrated into mathematical models to improve the accuracy of predictions regarding community transmission and the spread of drug resistance under varying treatment guidelines.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Burden of Latent Tuberculosis Infection among Healthcare Workers of Primary Health Centres and its Association with Clinical Laboratory Parameters\nAbstract: Objectives Healthcare workers (HCWs) are at high risk of latent tuberculosis infection (LTBI) due to their continued occupational exposure. The incidence of LTBI among the HCWs has seldom been investigated. Primary health centres (PHCs) provide effective and affordable medical care largely for the local population. The HCWs of PHCs who are likely to have increased occupational exposure have an increased risk of reactive as well as LTBI, and contribute to overall TB transmission than the general population. Methods A cross-sectional study (March-April 2024) was carried out to assess the burden of LTBI among the HCWs of PHCs (n=64) across Chennai, India. Blood samples were analyzed for gamma interferon using a QuantiFERON-TB Gold Plus assay. A complete baseline health profile, including complete blood count, blood glucose, liver and renal functions, and ferritin levels was estimated to identify potential biomarkers that are independently associated with LTBI. Results The present study revealed an LTBI prevalence of 25.3% (n=99) among PHC workers. The red cell distribution width was significantly associated with LTBI positivity among different biochemical and haematological parameters analyzed. Factors such as individuals age, underlying comorbid conditions (30.3%; n=30), and longer employment duration (28%; n=28) were significantly associated with IGRA positivity. IGRA positivity was significantly associated with decreased RDW-SD in females, overweight, and participants with O blood group. Conclusions The study reported a high prevalence of LTBI among HCWs of PHCs, which necessitates their periodical screening for the elimination of TB.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Effects of family planning counseling delivered during maternal healthcare on postpartum modern contraceptive uptake in low- and middle-income countries: a systematic review and meta-analysis\nAbstract: Background: Postpartum modern contraception is crucial for preventing unintended pregnancies, reducing short inter-pregnancy intervals, and improving maternal and child health outcomes. Family planning counseling, when engaging with maternal healthcare services, may enhance modern contraceptive uptake in the postpartum period. However, evidence in low- and middle-income countries (LMICs) has yielded conflicting findings. We aimed to explore the effects of family planning counseling delivered as part of maternal healthcare on postpartum modern contraceptive uptake in LMICs. Methods: The following six databases were searched in April 2024: PubMed, Web of Science, Embase, Global Health, Medline, and Scopus. Studies that examined the association between family planning counseling and modern contraception uptake in the postpartum period were included. The findings of the included studies were summarized narratively and through a random-effects meta-analysis when data supported. Trim and fill estimates and subgroup analyses were conducted to address publication bias and heterogeneity, respectively. Results: A total of 61 studies were included, of which 42 studies were considered for quantitative synthesis and the remaining 19 studies included in a narrative synthesis. We found that women who received family planning counseling during maternal healthcare visits were 2.75 times (95% CI, 2.11-3.58) more likely to use modern contraception during the postpartum period compared to those who did not receive family planning counseling. Although we observed significant publication bias and heterogeneity, addressing these issues through trim and fill estimation and subgroup analyses, respectively, produced estimates consistent with the summary findings. A narrative synthesis of an additional 19 studies also supports the effectiveness of family planning counseling during maternal healthcare visits on postpartum modern contraception uptake. Conclusion: Integrating family planning counseling into maternal healthcare can significantly increase modern contraceptive use in the postpartum period, as such effectively reducing unintended and short interval pregnancy as well as adverse maternal and child health outcomes. Policymakers should prioritize training healthcare providers and developing standardized protocols for effective counseling.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: SARS-CoV-2 Omicron XBB infections boost cross-variant neutralizing antibodies, potentially explaining the observed delay of the JN.1 wave in some Brazilian regions\nAbstract: The SARS-CoV-2 JN.1 lineage emerged in late 2023 and quickly replaced the XBB lineages, becoming the predominant Omicron variant worldwide in 2024. We estimate the epidemiological impact of this SARS-CoV-2 lineage replacement in Brazil and we further assessed the cross-reactive neutralizing antibody (NAb) responses in a cohort of convalescent Brazilian patients infected during 2023. We analyzed the evolution of SARS-CoV-2 lineages and severe acute respiratory infection (SARI) cases in Brazil between July 2023 and March 2024. We evaluated the cross-reactive NAb responses to the JN.1 variant in a cohort of convalescent Brazilian patients, both before and after infection with XBB.1* lineages. JN.1 replaced XBB with similar temporal dynamics across all country regions, though its epidemiological impact varied between locations. The Southeastern, Southern, and Central-Western regions experienced a brief XBB wave around October 2023, shortly before the introduction of JN.1, without any immediate upsurge of SARI cases during viral lineage replacement. By contrast, the Northeastern and Northern regions did not experience an XBB wave in the latter half of 2023 and displayed a rapid surge in SARI cases driven by the emergence of the JN.1. We found that recent XBB infections in the Brazilian population significantly boosted cross-reactive NAb levels against JN.1. The XBB wave observed in the second half of 2023 in some Brazilian states likely acted as a booster for population immunity, providing short-term protection against JN.1 infections and delaying the rise of SARI cases in certain regions of the country.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Gut colonization of Enterococcus species is associated with COVID-19 disease in Uganda\nAbstract: Background: Infection with the COVID-19-causing pathogen SARS-CoV-2 is associated with disruption in the human gut microbiome. The gut microbiome enables protection against diverse pathogens and exhibits dysbiosis during infectious and autoimmune disease. Studies based in the United States and China have found that severe COVID-19 cases have altered gut microbiome composition when compared to mild COVID-19 cases. We present the first study to investigate the gut microbiome composition of COVID-19 cases in a population from Sub-Saharan Africa. Given the impact of geography and cultural traditions on microbiome composition, it is important to investigate the microbiome globally and not draw broad conclusions from homogenous populations. Results: We used stool samples in a Ugandan biobank collected from COVID-19 cases during 2020-2022. We profiled the gut microbiomes of 114 symptomatic individuals who tested positive for SARS-CoV-2 along with 76 household contacts who did not present any symptoms of COVID-19. The inclusion of healthy controls enables us to generate hypotheses about bacterial strains potentially related to susceptibility to COVID-19 disease, which is highly heterogeneous. Comparison of the COVID-19 patients and their household contacts revealed decreased alpha diversity and blooms of Enterococcus and Eggerthella in COVID-19 cases. Conclusions: Our study finds that the microbiome of COVID-19 individuals is more likely to be disrupted, as indicated by decreased diversity and increased pathobiont levels. This is either a consequence of the disease or may indicate that certain microbiome states increase susceptibility to COVID-19 disease. Our findings enable comparison with cohorts previously published in the Global North, as well as support new hypotheses about the interaction between the gut microbiome and SARS-CoV-2 infection.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Design and evaluation of an automated real-time SOFA score calculator in an electronic health record system, for early sepsis diagnosis\nAbstract: Background: In patients with infections, the Sequential Organ Failure Assessment (SOFA) score should be used to diagnose sepsis. However, manual SOFA calculation is time consuming. Thus, we developed an automated real-time SOFA score application that has been fully implemented into the major electronic health record (EHR) system (TakeCare) in Stockholm. We aimed to describe the method and evaluate its performance for detection of sepsis. Method: We developed an automated SOFA application that presents a total score (SOFATotal) over time and a baseline score (SOFAChronic) based on outpatient data prior to hospital admission. We evaluated its performance on 583 sepsis alert patients in an emergency department, including 472 with sepsis according to manually calculated SOFA (SOFAManual). SOFATotal at 30 minutes and 3 hours, and quick-SOFA (qSOFA) were compared with SOFAManual for detection of sepsis. The acute SOFA score (SOFAAcute) was calculated by subtracting SOFAChronic from SOFATotal. Results: SOFATotal at 3 hours showed moderate-good agreement with SOFAManual (R2 = 0.76). Areas under receiver operating characteristic curves for sepsis detection were 0.83 at 30 minutes, 0.94 at 3 hours for SOFATotal, and 0.67 for qSOFA. Among patients with SOFAChronic [&ge;]1 (21% of cases), SOFATotal of [&ge;]2 was observed in 90% of cases (mortality 13.5%), although SOFAAcute of [&ge;]2 was noted in 63% (mortality 16.7%). Conclusion: The automated SOFA score demonstrated effectiveness in early sepsis detection, outperforming qSOFA, but adjustments for chronic baseline scores were necessary to avoid over-diagnosis. Further studies on broader patient populations would be useful to validate its clinical use.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Vaccine failure mode determines population-level impact of vaccination campaigns during epidemics.\nAbstract: Vaccines are a crucial tool for controlling infectious diseases, yet rarely offer perfect protection. \"Vaccine efficacy\" describes a population-level effect measured in clinical trials, but mathematical models used to evaluate the impact of vaccination campaigns require specifying how vaccines fail at the individual level, which is often impossible to measure. Does 90% efficacy imply perfect protection in 90% of people and no protection in 10% (\"all-or-nothing\"), or that the per-exposure risk is reduced by 90% in all vaccinated individuals (\"leaky\"), or somewhere in between? Here we systematically investigate the role of vaccine failure mode in controlling ongoing epidemics. We find that the difference in population-level impact between all-or-nothing and leaky vaccines can be substantial when R0 is higher, vaccines efficacy is intermediate, and vaccines slow but can't curtail an outbreak. Comparing COVID-19 pandemic phases, we show times when model predictions would have been most sensitive to assumptions about vaccine failure mode. When determining the optimal risk group to prioritize for limited vaccines, we find that modeling a leaky vaccine as all-or-nothing (or vice versa) can change the recommended target group. Overall, we conclude that models of vaccination campaigns should include uncertainty about vaccine failure mode in their design and interpretation.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Estimating the trend of COVID-19 in Norway by combining multiple surveillance indicators\nAbstract: Background: Estimating the trend of new infections was crucial for monitoring risk and for evaluating strategies and interventions during the COVID-19 pandemic. The pandemic revealed the utility of new data sources and highlighted challenges in interpreting surveillance indicators when changes in disease severity, testing practices or reporting occur. Aim: To estimate the trend in new COVID-19 infections by combining estimates of growth rates from all available surveillance indicators in Norway. Methods: We estimated growth rates from ten different surveillance indicators in Norway by using a negative binomial regression method and aligned the growth rates in time to hospital admissions by maximising correlations. Using a meta-analysis framework, we calculated overall growth rates and reproduction numbers, including assessments of the heterogeneity between indicators. Results: The estimated growth rates reached a maximum of 25% per day in March 2020, but afterwards they were between -10% and 10% per day. The correlations between the growth rates estimated from different indicators were between 0.5 and 1.0. Growth rates from indicators based on wastewater, panel and cohort data can give up to 14 days earlier signals of trends compared to hospital admissions, while indicators based on positive lab tests can give signals up to 7 days earlier. Conclusions: Combining estimates of growth rates from multiple surveillance indicators provides a good description of the COVID-19 pandemic in Norway. This is a powerful technique for a holistic understanding of the trends of new COVID-19 infections and the technique can easily be adapted to new data sources and situations.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Development, Testing, and Calibration of LINCS: A New Microsimulation Model of Maternal and Fetal Cytomegalovirus Infection\nAbstract: Background: Congenital cytomegalovirus (cCMV) is a leading cause of birth defects and the most common cause of non-genetic sensorineural hearing loss in children. There is a lack of decision modeling frameworks that can project cytomegalovirus (CMV) related patient outcomes and inform health policy. We created, tested, and calibrated a model of CMV acquisition and transmission in pregnancy using linked mother-infant dyads. Methods: We developed the Linking INfants and Mothers in Cytomegalovirus Simulation (LINCS) dyad-level Monte-Carlo microsimulation model of CMV infection among pregnant people and fetuses throughout pregnancy. We parameterized the model with data from the existing literature, implemented rigorous code testing procedures, and calibrated a key set of parameters to match model output to external data on cCMV prevalence and symptom risk. Results: A fully parameterized model for CMV among pregnant people and fetuses was developed, and the model code was confirmed to perform as specified. The calibration procedure identified parameter sets that generated model output closely matching the target values from the available data on cCMV prevalence and symptom risk. Conclusions: The LINCS model's ability to simulate the natural history of CMV infection during pregnancy was described and demonstrated, and the model was tested and calibrated to ensure proper functioning. Base case parameters were derived for CMV infection natural history to be used in future decision analyses of CMV testing and treatment strategies.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Longitudinal co-development of mental and cardio-metabolic health from childhood to young adulthood\nAbstract: Objective: Depressive symptoms and cardio-metabolic risk factors often co-occur. However, our understanding of the potential mechanisms and temporal dynamics underlying their co-development remains elusive. Method: This population-based cohort study examined bidirectional longitudinal associations between depressive symptoms and cardio-metabolic risk factors from age 10 to 25 years, using prospective data from the ALSPAC Study. Participants with at least one (of six) follow-up measurement for each outcome were included in the analyses. We measured depressive symptoms through self- as well as parent-reports, and assessed several cardio-metabolic risk factors (including adiposity measures, lipid profiles and inflammation). Results: Among our 7970 (47% male) participants, we found bidirectional, within-person associations between self-reported depressive symptoms and adiposity (i.e., fat/lean mass index, but not body mass index), across the study period. Adiposity was more stable over time ({beta} [range] = 0.75 [0.54; 0.84]), compared to depressive symptoms (0.26 [0.12; 0.38]), and it had a stronger prospective (i.e., cross-lagged) association with future depressive symptoms (0.07 [0.03, 0.13]) compared to that between depressive symptom and future adiposity (0.04 [0.03, 0.06]). The magnitude of these associations reached its peak between 14 and 16 years. We did not find evidence of cross-lagged associations in either direction between depressive symptoms and waist circumference, insulin, triglycerides, LDL cholesterol or C-reactive protein. Conclusions: These findings suggest a bidirectional relationship between depressive symptoms and cardio-metabolic risk factors, particularly adiposity (i.e., fat/lean mass). Adiposity showed a stronger prospective association with future depressive symptoms, than vice versa, however their relationship revealed more reciprocal than previously thought.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Disrupted functional brain network associated with presence of hallucinations in Parkinsons Disease\nAbstract: Background Hallucinations negatively impact quality of life in Parkinsons disease, yet their neural mechanisms remain poorly understood, particularly in early disease stages. This study aimed to identify functional connectivity differences associated with visual hallucinations in early Parkinsons Disease and to validate these findings across independent datasets. Methods Resting state fMRI data from two prior independent studies was used (total N=185; N=84 hallucinators and N=101 non-hallucinators). Group differences in functional connectivity were assessed within predefined cytoarchitectonic cortical classes and functional networks, followed by whole-brain analysis using Network-Based Statistics (NBS). Associations with clinical measures, including hallucination severity, motor symptoms, cognition, and attention, were also evaluated. Results NBS identified a subnetwork of reduced functional connectivity in hallucinators, connecting regions involved in the default mode, somatomotor and attentional networks. This subnetwork was replicated in a matched sample from our independent cohort (N=50; p<0.01). Functional connectivity within the identified network was significantly associated with hallucination severity (Rsquare = 0.35, p = 0.01), and with baseline and future motor symptoms, cognition, and attention in hallucinators. Conclusions The identified functional subnetwork shows promise as a potential biomarker and therapeutic target for Parkinsons disease psychosis, warranting further investigation and validation in future studies.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Performance of rapid diagnostic tests, microscopy, and qPCR for detection of parasites among community members with or without symptoms of malaria in villages with high levels of artemisinin partial resistance in North-western Tanzania\nAbstract: Background: Despite the implementation of different control interventions, infections in the communities (among asymptomatic and symptomatic individuals) still play a crucial role in sustaining malaria transmission. This study evaluated the performance of rapid diagnostic tests (RDTs), microscopy, and quantitative PCR (qPCR) in detecting malaria parasites among community members in five villages of Kyerwa district, Kagera region, an area where artemisinin partial resistance (ART-R) has been recently confirmed. Methods: A community cross-sectional survey of asymptomatic and symptomatic participants (n=4454) aged [&ge;]6 months was conducted in July and August 2023. Malaria infections were detected using RDTs, microscopy, and qPCR (using the 18S RNA gene). Performance of RDTs and microscopy was assessed by sensitivity, specificity, and predictive values, using qPCR as the reference method. Factors affecting the accuracy of these methods were explored using a multivariate logistic regression model. Results: The prevalence of malaria infections was 44.4% (n = 1979/4454), 32.1% (n = 1431/4454), and 39.8% (n = 1771/4454) by RDTs, microscopy, and qPCR, respectively. The prevalence of P. malariae and P. ovale mono-infection by microscopy was 0.2% (n = 7/4454) and 0.3% (n = 12/4454), while by qPCR was 0.4% (n = 16/4454) and 0.5% (n = 24/4454), respectively. The geometric mean parasite densities (GMPDs) by microscopy were 642 (95% confidence intervals (CI) = 570 - 723), 126 (95% CI = 98 - 162), and 124 (95% CI = 82 - 160) asexual parasites/[micro]L; while by qPCR, the GMPDs were 1180 (95% CI = 1032 - 1349), 44 (95% CI = 32 - 61), and 50 (95% CI = 29 - 89) parasites/[micro]L for P. falciparum, P. ovale spp, and P. malariae, respectively. The sensitivity and specificity of RDTs were 94.0% (95% CI = 92.8% - 95.1%) and 87.5% (95% CI = 86.2% - 88.7%), respectively; whereas those of microscopy were 74.6% (95% CI = 72.5% - 76.6%) and 95.2% (95% CI = 94.3% - 96.0%), respectively. The sensitivity of RDTs and microscopy was low at very low parasitemia (<100 parasite/[micro]L), but increased significantly with increasing parasitemia, reaching [&ge;]99.6% at >10000 parasites/{micro}L (p<0.001). Conclusion: A higher prevalence of malaria was detected and the performance of RDTs and qPCR were comparable, but microscopy had a lower performance. Higher sensitivity by RDTs compared to microscopy, indicates that RDTs are effective for the detection of malaria infections for routine case management and surveillance of malaria in this area with confirmed ART-R; and can be utilized in the ongoing plans to develop a response to ART-R.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The Lifecycle of Electronic Health Record Data in HIV-Related Big Data Studies: A Qualitative Study of Instances of and Potential Opportunities to Minimize Bias\nAbstract: Background: Electronic health record (EHR) data are widely used in public health research, including HIV-related studies, but are limited by potential bias due to incomplete and inaccurate information, lack of generalizability, and lack of representativeness. This study explores how workflow processes within HIV clinics, among data scientists, and within state health departments may introduce and minimize bias within EHRs. Methods: Using a constructivist grounded theory approach, in-depth individual interviews were conducted with 16 participants purposively sampled in South Carolina from August 2023-April 2024. A focus group with 3 health department professionals with expertise in HIV disease surveillance was also conducted. Analysis was conducted as outlined by Charmaz (2006). Results: To reduce bias in EHR data, information entry forms should be designed to expansively include patient self-reported social determinants of health (SDOH) information. During data collection, healthcare providers should create a supportive healthcare environment, facilitate SDOH information disclosure, and accurately document patient information. Patients should have access to their EHRs to confirm that SDOH information are correctly recorded. During data curation, data scientists should inspect datasets for completeness, accuracy, and educate public health researchers on dataset limitations. During data management and utilization, health department professionals should crossmatch data across the state, customize data collection systems to reflect local needs, and provide community-based data education and stigma management. Conclusion: Study results suggest that future research is needed to understand how healthcare systems can be incentivized to create and implement EHR bias reduction strategies across all workflows and between stakeholders.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Sustained attention processes in very preterm adolescents and their relationship to socio-emotional competence.\nAbstract: Very preterm (VPT) adolescents are at high risk of impaired sustained attention processes, as well as behavioral and socio-emotional problems. Previous studies have highlighted altered attentional patterns of brain activation in this population, but results are inconsistent. The current study aims to explore brain activity related to sustained attention in VPT and full-term adolescents aged 11 18, as well as its associations with attentional capacities and socio emotional competences. Event related functional MRI (fMRI) was used to assess sustained attention performance and associated brain activations by comparing VPT (n = 34) and their age matched full term (FT, n = 28) peers from a previously validated continuous performance task with gradual onset (gradCPT) paradigm, using two different modality versions (i.e., face and scene). In both groups, linear regression analyses were performed to examine associations between attentional and socio emotional difficulties and brain activations related to sustained attention. Results show preserved sustained attention processes in VPT adolescents, indicated by comparable behavioral attentional performance and cerebral patterns of activations in both groups across the two modalities of the gradCPT. In addition, VPT adolescents showed over recruitments in posterior occipital areas compared to FT adolescents. Moreover, higher socio emotional difficulties (i.e., higher anxiety and social difficulties) in VPT were linked to altered activations specifically in the right middle frontal gyrus, occipito temporal gyri and bilateral cerebellum, but exclusively observed during the face modality of the gradCPT. Overall, these results suggest that despite preserved sustained attention competences, VPT adolescents present a less mature sustained attention cerebral network, particularly during a task with a social context.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Investigation of the urinary peptidome to unravel collagen degradation in health and kidney disease\nAbstract: Naturally occurring fragments of collagen type I alpha 1 chain (COL1A1) have been previously associated with chronic kidney disease (CKD), with some fragments showing positive and others negative associations. Using urinary peptidome data from healthy individuals (n=1131) and CKD patients (n=5585) this aspect was investigated in detail. Based on the hypothesis that many collagen peptides are derived not from the full, mature collagen molecule, but from (larger) collagen degradation products, relationships between COL1A1 peptides containing identical sequences were investigated, with the smaller (offspring) peptide being a possible degradation product of the larger (parent) one. The strongest correlations were found for relationships where the parent differed by a maximum of 3 amino acids from the offspring, indicating an exopeptidase-regulated stepwise degradation process. Regression analysis indicated that CKD affects this degradation process. Comparison of matched CKD patients and control individuals (n=612 each) showed that peptides at the start of the degradation process were consistently downregulated in CKD, indicating an attenuation of COL1A1 endopeptidase-mediated degradation. However, as these peptides undergo further degradation, likely mediated by exopeptidases, this downregulation can become less significant or even reverse, leading to an upregulation of later stage fragments and potentially explaining the inconsistencies observed in previous studies.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Identification of novel nutrient-sensitive human yolk sac functions required for embryogenesis\nAbstract: The human yolk sac (hYS) is essential for embryo nutrient biosynthesis/transport and development. However, there lacks a comprehensive study of hYS nutrient-gene interactions. Here we performed a secondary analysis of hYS transcript profiles (n=9 samples) to identify nutrient-sensitive hYS genes and regulatory networks, including those that associate with adverse perinatal phenotypes with embryonic origins. Overall, 14.8% highly expressed hYS genes are nutrient-sensitive; the most common nutrient cofactors for hYS genes are metals and B-vitamins. Functional analysis of highly expressed hYS genes reveals that nutrient-sensitive hYS genes are more likely to be involved in metabolic functions than hYS genes that are not nutrient-sensitive. Through nutrient-sensitive gene network analysis, we find that four nutrient-sensitive transcription regulators in the hYS (with zinc and/or magnesium cofactors) are predicted to collectively regulate 30.9% of highly expressed hYS genes. Lastly, we identify 117 nutrient-sensitive hYS genes that associate with an adverse perinatal outcome with embryonic origins. Among these, the greatest number of nutrient-sensitive hYS genes are linked to congenital heart defects (n=54 genes), followed by microcephaly (n=37). Collectively, our study characterises nutrient-sensitive hYS functions and improves understanding of the ways in which nutrient-gene interactions in the hYS may influence both typical and pathological development.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Adverse respiratory events during treatment with gabapentin and opioids among older adults with spine-related conditions: a propensity-matched cohort study in the US Medicare population\nAbstract: Background Context: Recent work indicates no increased mortality risk with concurrent gabapentin and opioid use when using an active comparator control design. However, concurrent gabapentin and opioid prescriptions have been associated with greater risk of respiratory depression in some studies. Purpose: To compare the risk of respiratory events among Medicare enrollees with spine-related diagnoses treated with gabapentin + opioids vs those treated with tricyclic antidepressants (TCA) or duloxetine + opioids. We hypothesized that enrollees treated with gabapentin + opioids would have increased risk of adverse respiratory events compared to those treated with an active control + opioids. Study Design/Setting: Propensity score-matched cohort study with an incident user, active comparator (TCA/duloxetine) control design. The primary analysis included those who concurrently (within 30 days) filled [&ge;]1 incident gabapentin + [&ge;]1 opioid or [&ge;]1 incident TCA/duloxetine + [&ge;]1 opioid prescription. Patient Sample: U.S. Medicare beneficiaries with spine-related diagnoses 2017-2019. People treated with gabapentin + opioids (n=66,580) were matched on demographic and clinical factors to people treated with TCAs/duloxetine + opioids (n=66,580). Outcome Measures: Time to a composite respiratory outcome consisting of mechanical ventilation, intubation, respiratory failure, pneumonia, or acute respiratory distress syndrome. Methods: Cox proportional hazard regression was used to estimate adjusted hazard ratios (aHRs) and 95% confidence intervals (95% CIs). Results: Among 133,160 Medicare enrollees (median age 73.3 years; 66.7% female), 6089 (4.6%) experienced respiratory events before the end of follow-up. A total of 3297 (5.0%) of people who were treated with gabapentin + opioids (median initial dose/day of gabapentin was 338 mg) had respiratory events compared to 2792 (4.2%) of those treated with an active control + opioids. The increased risk in those treated with gabapentin + opioids was statistically significant after adjustment (HR 1.14; 95% CI 1.09, 1.20; p<0.0001). The most common respiratory events were pneumonia (3.5% of people in the gabapentin + opioids group versus 3.0% of people in the TCA/duloxetine + opioids group) and respiratory failure (2.2% in the gabapentin + opioids group versus 1.8% in the TCA/duloxetine + opioids group). Results were similar in analyses (a) restricted to [&le;]30-day follow-up and (b) that required [&ge;]2 fills of each prescription. Conclusions: While recent work has indicated no increased mortality risk with concurrent gabapentin and opioid use, the current findings suggest clinicians should exercise caution in prescribing gabapentin to people experiencing pain who are also being treated with opioids, due to the potentially increased risk of respiratory events.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The neurodevelopmental spectrum of CASK-related disorder\nAbstract: Background: Pathogenic CASK variants are associated with neurodevelopmental disorders of variable severity including X-linked intellectual disability (XLID) and microcephaly with pontocerebellar hypoplasia (MICPCH). Although the number of diagnosed cases is rising, current understanding of the CASK-related neurodevelopmental spectrum is limited. Here, we systematically review the published characteristics of individuals with CASK-related disorder, and compare these to a more recently-diagnosed group. We provide quantitative information about the ranges of adaptive abilities, motor function, visual function and social-emotional-behavioural characteristics, and explore within-group associations. Methods: 151 individuals with CASK variants were identified in published literature. 31 children and young people with CASK variants were recruited to the UK-based Brain and Behaviour in Neurodevelopmental disorder of Genetic Origin (BINGO) project. BINGO-participating caregivers completed a bespoke medical history questionnaire and battery of standardised neurodevelopmental measures. Results: Comparing the recently diagnosed BINGO CASK-related disorder group to previously reported individuals, we found consistent prevalence of tone abnormalities, sensorineural hearing loss and epilepsy, but lower prevalence of severe/profound ID, MICPCH, optic atrophy and nystagmus. Areas of frequent difficulty not highlighted in previous reports include sleep difficulties and cerebral visual impairment (CVI). Neurodevelopmental characteristics were highly variable within the BINGO CASK-related disorder group, and group-wide patterns were similar to those observed in other rare genetic conditions. Within the BINGO CASK-related group, epilepsy is significantly associated with ID severity, after controlling for age. Sub-groups with MICPCH or microcephaly only have equivalent ranges of adaptive function, but MICPCH may be associated with more severe motor difficulties. Conclusion: The spectrum of neurodevelopmental characteristics associated with CASK-related disorder appears to be broadening with increased access to genome-wide diagnostic testing. Further studies are needed to elucidate the relationships between CASK variants, structural brain development, epilepsy, and neurodevelopmental characteristics.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Long-Read Sequencing Increases Diagnostic Yield for Pediatric Sensorineural Hearing Loss\nAbstract: The diagnostic yield of genetic testing for pediatric sensorineural hearing loss (SNHL) has remained at around 40% for over a decade despite newly discovered causative genes and the expanded use of exome sequencing (ES). This stagnation may be due to (1) a focus on coding regions of the genome and (2) an inability to resolve variants in complex genomic regions due to reliance on short-read sequencing technologies. Short-read genome sequencing (srGS) and long-read genome sequencing (lrGS) both provide exonic single nucleotide variant (SNV) and small indel detection at the same sensitivity as ES, but also evaluate intronic regions. lrGS provides improved resolution for structural variants (SV) and repetitive genomic regions. We sought to investigate the potential utility of lrGS in the diagnostic evaluation of a small cohort of patients with SNHL of unknown etiology after ES and srGS. 19 pediatric patients with SNHL underwent lrGS via PacBio SMRT sequencing. Sequencing data were processed using the PacBio WGS variant pipeline. The diagnostic yield for this lrGS cohort was 4/19 (21%). Relevant variants detected only with lrGS included a hemizygous deletion in trans with a missense variant in an area of high genomic homology (OTOA) and two single nucleotide loss-of-function variants in trans to a known copy-number-loss for a gene with a highly homologous pseudogene (STRC). A complex inversion was identified in the MITF gene which was also identified on post-hoc analysis by srGS. LrGS provides improved resolution for complex genomic structural variation which may increase diagnostic yield for genetic pediatric SNHL, and, potentially, rare disease more broadly.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The contribution of coding variants to the heritability of multiple cancer types using UK Biobank whole-exome sequencing data\nAbstract: Genome-wide association studies have been highly successful at identifying common variants associated with cancer, however, they do not explain all the inherited risk of cancer. Family-based studies, targeted sequencing and, more recently, exome-wide association studies have identified rare coding variants in some genes associated with cancer risk, but the overall contribution of these variants to the heritability of cancer is less clear. Here we describe a method to estimate the genome-wide contribution of rare coding variants to heritability that fits models to the burden effect sizes using an empirical Bayesian approach. We apply this method to the burden of protein-truncating variants in over 15,000 genes for 11 cancers in UK Biobank, using whole-exome sequencing data on over 400,000 individuals. We extend the method to consider the overlap of genes contributing to pairs of cancers. We found ovarian cancer to have the greatest proportion of heritability attributable to protein-truncating variants in genes (46%). The joint cancer models highlight significant clustering of cancer types, including a near complete overlap in susceptibility genes for breast, ovarian, prostate and pancreatic cancer. Our results provide insights into the contribution of rare coding variants to the heritability of cancer and identify additional genes with strong evidence of susceptibility to multiple cancer types.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Pretrained Patient Trajectories for Adverse Drug Event Prediction Using Common Data Model-based Electronic Health Records\nAbstract: Pretraining electronic health record (EHR) data through language models by treating patient trajectories as natural language sentences has improved various medical tasks. However, EHR pretraining models have never been utilized in adverse drug event (ADE) prediction. Here, we propose a novel pretraining scheme for common data model (CDM) based EHR data, named CDM-BERT. We utilized diagnosis, prescription, measurement, and procedure domains from observational medical outcomes partnership (OMOP)-CDM. We newly adopted domain embedding (DE) to simplify pretraining procedure and to improve comprehension of medical context. ADE prediction was selected as a finetuning task. For drug groups, we included nonsteroidal anti-inflammatory drugs (NSAID), anticoagulants (AC), glucocorticoids (GC), and chemotherapy (Chemo). For corresponding adverse events, we selected peptic ulcer (PU), intracranial hemorrhage (ICH), osteoporosis (OP), and neutropenic fever (NF), respectively. CDM-BERT was validated by internal and external datasets with 510,879 and 419,505 adult inpatients. CDM-BERT outperformed all the other baselines in all cohorts, demonstrating the effectiveness of DE (area under the receiver operating characteristic curve (AUROC) of 0.977, 0.908, 0.980, 0.989 for NSAID-PU, AC-ICH, GC-OP, Chemo-NF cohorts in internal validation, and 0.967, 0.960, 0.972, 0.959 in external validation, respectively). We also identified important features for each cohort, and several prior studies and clinical knowledge suggested the results. CDM-BERT has demonstrated its potential as a foundation model through its prediction performance, interpretability, and compatibility.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: I answer to clinical scenarios as a real doctor because I \"Think\": \"ChatGPTo1\" new reasoning ability.\nAbstract: The adoption of the ChatGPT o1 model represents a significant advancement in the management of clinical cases due to the introduction of a new structured reasoning capability, the \"chain-of-thought reasoning\" (CoT). In this study, 350 general medicine clinical cases were tested using ChatGPT o1 and ChatGPT o1 mini, and their performance was compared with ChatGPT 4o and ChatGPT 4o mini to evaluate diagnostic accuracy. The results showed that ChatGPT o1 achieved a correct answer rate of 93.4%, outperforming both ChatGPT 4o (82.2%) and the mini versions (ChatGPT o1 mini: 70.2% and ChatGPT 4o mini: 66.2%). The CoT technique enabled the model to provide more coherent and transparent responses, reducing the occurrence of so-called \"hallucinations.\" This study highlights how the ChatGPT o1 model can be a valuable tool in clinical practice, although its use requires supervision to ensure patient safety, especially in critical settings.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Machine learning framework for predicting the presence of high-risk clonal haematopoiesis using complete blood count data: a population-based study of 431,531 UK Biobank participants\nAbstract: Background Clonal haematopoiesis (CH), the disproportionate expansion of a haematopoietic stem cell and its progeny, driven by somatic DNA mutations, is a common age-related phenomenon that engenders an increased risk of developing myeloid neoplasms (MN). At present, CH is identified by targeted sequencing of peripheral blood DNA, which is impractical to apply at population scale. The complete blood count (CBC) is an inexpensive, widely used clinical test. Here, we explore whether machine learning (ML) approaches applied to CBC data could predict individuals likely to harbour CH and prioritise them for DNA sequencing. Methods The UK Biobank was filtered to identify 431,531 participants with paired CBC and whole exome sequencing (WES). Somatic mutations were previously identified from blood WES using Mutect2 to classify individuals with CH driver mutations. Using 18 CBC indices/features and basic demographics (age and sex), we trained a range of tree-based ML classifiers to infer as binary output, the presence/ absence of CH. Findings Using Random Forest (RF) classifiers, we predicted the presence/absence of CH driven by mutations in one of five genes known to confer a high-risk of incident MN (JAK2, CALR, SF3B1, SRSF2 and U2AF1). We subsequently developed a unified, optimised RF classifier for high-risk CH driven by any of these genes and assessed its performance (median AUC 0.85). However, the low prevalence of high-risk CH implies that our model cannot be generalised to population scale without compromising its sensitivity (20.1% using stringent cutoff probability score). Interpretation We showcase a proof-of-concept that the presence of high-risk CH can be inferred from CBC perturbations using RF classifiers. The future integration of raw blood cell analyser data can help improve the performance of our model and facilitate its application at scale.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Risk of cardiovascular disease in patients with classical Hodgkin lymphoma: a Danish nationwide register-based cohort study\nAbstract: The risk of cardiovascular disease (CVD) in patients with classical Hodgkin lymphoma (cHL) undergoing contemporary treatment is unclear. Methods: All patients with cHL [&ge;]18 years at diagnosis treated with doxorubicin-containing chemotherapy between 2000-2022 were matched with comparators in a 1:5 ratio on birth year, sex, and Charlson Comorbidity Index at time of matching (score of 0 or [&ge;]1). The cause-specific cumulative incidence of a composite of CVDs and corresponding 95% confidence intervals (CIs) were computed with death and lymphoma relapse as competing events using the Aahlen-Johansen estimator. Results: A total of 1,905 patients and 9,525 comparators with a median follow-up of 10 years (interquartile range, [IQR]: 5.9-17.4). Median age was 39 years (IQR: 27-56), median cumulative doxorubicin dose was 250 mg/m2 (IQR: 200-300). The CVD cumulative incidences were 4.7% (95% CI: 3.6-5.7) for patients vs. 2.6% (95% CI: 2.3-2.9) for comparators at 5 years, 8.9% (95% CI: 7.2-10.5) vs. 5.5% (95% CI: 4.9-6.0) at 10 years, and 17.0% (95% CI: 14.1-19.9) vs. 8.2% (95% CI: 7.4-9.0) at 15 years. Conclusions: CVD remains a substantial effect after contemporary treatment for cHL, suggesting that awareness of symptoms and a low threshold for referral to diagnostic examination are still important measures during survivorship.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Acceptability and implementation considerations of the patient-collected rectal swab for sexually transmitted infections testing among men who have sex with men and transgender women in Kigali, Rwanda\nAbstract: Background Rectal sexually transmitted infections (STI) are prevalent among men who have sex with men (MSM) and transgender women (TGW). Self-collection of rectal specimens is widely used globally, but limited data exist on its implementation in Africa. We report experiences of MSM/TGW self-collecting rectal STI specimens in Kigali. Methods From March to August 2018, 738 MSM/TGW were recruited in a cross-sectional study using respondent-driven sampling in Kigali. We tested for Neisseria gonorrhea and Chlamydia trachomatis using the Cepheid GeneXpert CT/NG platform on self-collected rectal swabs. Likert scales assessed self-collection difficulty and comfort. Ordered logistic regression analyses were performed to characterize factors associated with self-collection difficulty. Results Overall, 14%(106) identified as TGW. In total, 78%(577) found rectal swab self-collection easy/very easy, while 15%(108) found it difficult/very difficult. Most, 92%(679), were comfortable/very comfortable with the test, and 98%(730) said they would repeat the test in the future. In multivariable RDS-adjusted analyses, discomfort with the swab was positively associated with difficulty in self-collection (adjusted cumulative odds ratios ((aCOR): 7.9(95%CI: 4.9-12.8)) and having a prevalent rectal STI (aCOR: 4.19, 95%CI: 2.02-8.72) was significantly associated with difficulty. Furthermore, 10%(75) of rectal swabs returned indeterminate results (65 invalid results and 10 errors). Conclusion Most MSM/TGW found self-collection of rectal swabs easy, comfortable, and collected valid samples. Self-collection of rectal swabs could be used alongside clinic- and community-based STI testing to improve diagnosis and treatment in Rwanda. However, measures to optimize sample collection and processing are needed to reduce the cost and clinical implications of indeterminate results\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Altered association of plasmatic oxytocin with affective stress responses in alcohol use disorder\nAbstract: Oxytocin has been proposed to play a role in the development and maintenance of alcohol use disorder (AUD) through its interactions with stress pathways. Empirical evidence that indicates altered associations between endogenous oxytocin and stress reactivity in AUD is currently lacking. In this study, we investigated baseline plasmatic oxytocin concentrations of early-abstinent patients with AUD (N = 40) and matched healthy control participants (N = 37), who completed the Trier Social Stress Test (TSST) as well as a control task on two separate visits. We measured salivary cortisol and pulse rate as indicators of a physiological stress response, and anxiety ratings as an indicator of an affective stress response at multiple time points. Baseline oxytocin levels did not significantly differ between the groups. However, our results suggest an altered association of oxytocin and affective stress responses in participants with AUD: while participants with AUD showed a positive relationship between plasmatic oxytocin levels and stress-induced anxiety increase, the opposite relationship was found in control participants. We did not find evidence for an association of oxytocin with physiological stress responses. Assuming that affective stress responses mediate addiction-related behaviors like craving and relapse, our findings suggest that administering exogenous oxytocin might not be beneficial in treating AUD, at least during the early stages of abstinence.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Bridging Cultures to Defeat COVID-19: An Innovative Virtual Exchange Program in Global Medical Education\nAbstract: The problem and opportunity: There is a critical and growing need to train globally focused, culturally fluent clinicians and scientists who can collaboratively defeat current and future public health threats across international boundaries. In parallel, academic conferences bring together thousands of diverse international healthcare professionals every year, yet their potential to provide the crucial professional development training necessary to advance internationalized medicine is often underutilized. The solution: We developed and now first report an innovative healthcare education program that used an academic conference as the framework around which to build a structured, non-incidental virtual exchange (VE) for training globally and culturally proficient healthcare professionals. Herein we further describe the program's design and content, successes and challenges, and lessons learned. Program Overview: Using a smartphone based social-networking and conference management app with available translation capabilities, pre- and post-graduate trainees prepared and participated in poster presentations, seminars, and workshops to learn current research and best-practices in COVID-19 medicine, while engaging with their international peers in networking and professional-development exercises. The 2-week intensive program included daily synchronous interactive seminars on various topics in COVID-19 medicine, international team-based asynchronous activities such as preparing, presenting, and constructively critiquing research posters at virtual poster sessions, and expert-led wellness and cultural-competence workshops. Participants received initial training in the norms of intercultural communication, syllabus content and expectations, incentives, icebreaker activities, and program technology. They learned then-current COVID-19 medical research, therapies, and best practices, as well as professional \"soft skills\" including leadership, team building, scientific/clinical presentation, verbal/written communication skills, and intercultural competence. The program vastly expanded participants' international professional networks to enhance their mentorship and career development opportunities. Conclusions: Participants reported receiving substantial benefits from the program, with many reporting immediate translation of lessons learned toward improving healthcare education or practice in their home communities.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Impaired speaking-induced suppression in schizophrenia predicts auditory verbal hallucinations\nAbstract: A successful efference copy self-prediction suppresses auditory signals in the primary auditory cortex (A1) is necessary for speakers to successfully compare auditory feedback during speech production with auditory feedback during passive listening, this is called speaker-induced suppression (SIS). The top-rank positive symptom in schizophrenic (SZ) patients, auditory verbal hallucination, for instance, is hypothesized to relate to failure to distinguish the internal voice and external sounds, and this deficit is thought to be associated with impaired self-prediction in comparing external and self-generated contents. In this magnetoencephalographic imaging (MEGI) study, we compared SIS M100 in the primary auditory cortex (A1) between the healthy controls (HC; N = 30) and SZ patients (N = 22). The SZ patients displayed reduced SIS and M100 in the A1, and this impairment is negatively correlated with auditory hallucinations. These outcomes suggest that the SZ patients' hallucinatory symptoms are caused by misattribution between the external and self-generated stimuli. We proposed that the weakened self-agency and neural oscillations may lead to this misattribution.\n"}, {"role": "assistant", "content": "no"}]}
