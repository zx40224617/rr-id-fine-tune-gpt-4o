{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The post-pandemic hospital and mortality burden of COVID-19 compared with influenza: A national cohort study in Denmark, May 2022 to June 2024\nAbstract: BackgroundIn the post-pandemic period, COVID-19 continues to cause significant numbers of hospitalisations and deaths. We describe this burden and compare it to the burden of influenza in the first two post-pandemic years in Denmark.  MethodsA cohort study including residents in Denmark from May 16, 2022, to June 7, 2024. Data were obtained from national registries, including information on Polymerase chain reaction (PCR) test-positive COVID-19 and influenza admissions, mortality within 30 days of admission, sex, age, COVID-19 and influenza vaccination, comorbidity, and living in long-term care facility for elderly. Negative binomial regression was used to estimate adjusted incidence rate ratios (aIRRs) to compare rates of admissions between COVID-19 and influenza. To assess severity of COVID-19 among hospitalized patients, we used Cox proportional hazard models to estimate adjusted hazard ratios (aHR) of 30-day mortality between COVID-19 and influenza.  ResultsAmong 5,899,170 individuals, admissions with COVID-19 (n=24,687) were more frequent than admissions with influenza (n=8,682; aIRR 2.01, 95%CI 1.37-2.95), in particular during the first year (p=0.01), in the summer (p<0.001) and among people above 65 years of age (p<0.001). The number of deaths were also higher (COVID-19, n=2,393; influenza, n=522). Among patients, the risk of mortality of COVID-19 was higher than influenza in the 12-30 days following admission (0-11 days, aHR 1.08, 95%CI 0.94-1.25; 12-30 days, aHR 1.50, 95%CI 1.21-1.84), in particular among non-vaccinated for both COVID-19 and influenza (aHR 1.81, 95%CI 1.25-2.62), while it was similar to influenza among patients without comorbidities (aHR 1.07, 95%CI 0.63-1.80).  ConclusionCOVID-19 represented a greater disease burden than influenza, with more hospitalisations and deaths, and more severe disease primarily among non-vaccinated and comorbid patients. These results highlight the continued need for attention and public health efforts to mitigate the impact of SARS-CoV-2.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Suicide Death Predictive Models using Electronic Health Record Data\nAbstract: In the realm of medical research, particularly in the study of suicide risk assessment, the integration of machine learning techniques with traditional statistics methods has become increasingly prevalent. This paper used data from the UNC EHR system from 2006 to 2020 to build models to predict suicide-related death. The dataset, with 1021 cases and 10185 controls consisted of demographic variables and short-term informa-tion, on the subjects prior diagnosis and healthcare utilization. We examined the efficacy of the super learner ensemble method in predicting suicide-related death lever-aging its capability to combine multiple predictive algorithms without the necessity of pre-selecting a single model. The study compared the performance of the super learner against five base models, demonstrating its superiority in terms of cross-validated neg-ative log-likelihood scores. The super learner improved upon the best algorithm by 60% and the worst algorithm by 97.5%. We also compared the cross-validated AUCs of the models optimized to have the best AUC to highlight the importance of the choice of risk function. The results highlight the potential of the super learner in complex predictive tasks in medical research, although considerations of computational expense and model complexity must be carefully managed.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Associations of Smoking Status and Leisure-time Physical Activity with Waist Circumference Change Ten-year Follow-up among Twin Adults\nAbstract: BACKGROUNDThis follow-up study investigated the associations of smoking status and leisure-time physical activity (LTPA) with weight circumference (WC) change.  METHODSIn the FinnTwin16 cohort, 3,431 twins (47% men) reported smoking status, LTPA, and WC in early adulthood and 10 years later. Regression models were conducted to investigate associations of smoking status and of LTPA change (metabolic equivalent tasks [MET]-h/week) with WC change (cm) during the follow-up. Within-pair associations were analyzed using linear mixed fixed-effect regression among 800 same-sexed (409 identical) pairs.  RESULTSDuring the 10-year follow-up, 40% (n=454) quit smoking. Among those who quit smoking, the mean WC increase was 7.4 cm (SD 8.2) and the mean LTPA decrease was -0.02 MET-h/week (SD 35.8). Compared to individuals continuing daily smoking, only quitters who smoked daily at baseline ({beta} 1.87; 95% CI 0.68, 3.06) increased their WC. This association was not robust after shared familial influences were controlled for. Each additional MET-h/week lowered the risk for WC increase among individuals who smoked occasionally ({beta} -0.054; -0,08, -0.003), quitters who smoked daily ({beta} -0.05; -0.06, -0.02) and those who had never smoked ({beta} -0.04; -0.05, -0.03). In the analyses among identical twin pairs, LTPA was associated with less WC increase among those who quit occasional smoking or had never smoked. For quitters from daily smoking, this association approached significance, but no association remained for those continuing smoking.  CONCLUSIONSSmoking cessation seems to be associated with WC increase, but familial confounding is involved in this process. LTPA may inhibit post-cessation WC increase.  KEY MESSAGESThis will be published as a summary box after the abstract in the final published article.  What is already known on this topic1. Individuals quitting smoking tend to gain weight, but less is known of post-cessation abdominal obesity and the role of leisure-time physical activity in this process.   What this study adds2. Smoking cessation associates with moderate increase in waist circumference among those who quit daily smoking, but shared familial influences are involved in this process. 3. Increase in leisure-time physical activity may prevent post-cessation waist circumference increase.   How this study might affect research, practice, or policy4. More research is needed to unravel genetic and epigenetic mechanisms underlying post-cessation waist circumference increase. 5. In practice, individuals who quit smoking should be encouraged to be physically active.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Detection of diphtheria toxin-producing Corynebacterium ramonii in wounds of an urban, inner-city population in Vancouver, Canada, 2019-2023\nAbstract: We conducted a retrospective chart review and whole-genome sequencing of specimens with presumptive Corynebacterium ulcerans in Vancouver from July 2019 to July 2023. WGS confirmed 8/14 wound culture isolates as C. ramonii. Two distinct clusters of C. ramonii were identified with clinical presentations similar to cutaneous C. diphtheriae.  Article SummaryTwo distinct clusters of C. ramonii were identified in Vancouver, BC with clinical presentations similar to cutaneous C. diphtheriae.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Synergizing Health Strategies: Exploring the Interplay of Treatment and Vaccination in an Age-Structured Malaria Model\nAbstract: Malaria remains a persistent global challenge, particularly prevalent in tropical regions of Africa, Asia, and South America. According to the World Health Organizations (WHO) World Malaria Report 2023, there were approximately 249 million reported malaria cases in 2022 across 85 endemic countries, resulting in over half a million deaths. Progress towards global malaria eradication through antimalarial drugs has been slow, with case numbers increasing since 2015. Dihydroartemisinin (DHA), artesunate, and artemether, derivatives of artemisinin, are crucial components of modern antimalarial treatment. However, resistance to these drugs and their partners in Artemisinin Combination Therapy (ACT) has emerged in Southeast Asia, Africa, and South America. In 2021, the WHO recommended widespread use of the RTS,S malaria vaccine among children in endemic regions. To address these challenges, we develop an extended SEIR age-structured model incorporating malaria vaccination for children, drug-sensitive and drug-resistant strains, and interactions between human hosts and mosquitoes. Our research focuses on evaluating how malaria vaccination coverage influences disease prevalence and transmission dynamics. We derive the basic, intervention, and invasion reproduction numbers for both strains and conduct sensitivity analysis to identify key parameters affecting infection prevalence. Our findings reveal that model outcomes are primarily influenced by scale factors that reduce transmission and natural recovery rates for the resistant strain, as well as by drug treatment and vaccination efficacies, and mosquito death rates. Numerical simulations indicate that while treatment reduces the malaria disease burden, it also increases the proportion of drug-resistant cases. Conversely, higher vaccination efficacy correlates with lower infection cases for both strains. These results suggest that a synergistic approach, involving both vaccination and treatment, could effectively decrease the overall proportion of the population that is infected.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Epidemiological Analysis of Confirmed Mpox Cases in Burundi, July to September 2024\nAbstract: This study analyzes mpox cases in Burundi from July to September 2024, following the 34 introduction of Clade Ib. During this time, 25.4% of 607 samples tested positive via PCR. 35 Children under 15 comprised 55.2% of cases, with a higher proportion of female children testing 36 positive. Geographic analysis demonstrates case concentration in Bujumbura Mairie (59.1%). 37 These findings highlight the importance of age- and sex-specific interventions for outbreak 38 containment and the need for targeted public health strategies in Burundi.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Predictive Time Series Modeling of Malaria Incidence in India: A 20-Year Retrospective and 5-Year Forecast.\nAbstract: This study presents a predictive time series analysis of malaria incidence in India, utilizing a two-decade dataset. Malaria, caused by Plasmodium species and transmitted by Anopheles mosquitoes, poses a significant public health challenge in the WHO South-East Asia Region. We employed linear regression and ARIMA modeling techniques to analyze historical incidence data, ensuring rigorous data cleaning for accuracy. Results indicate a statistically significant declining trend in malaria cases, with projections suggesting continued decreases in malaria cases over the next five years. These findings highlight the effectiveness of current malaria elimination strategies in India and underscore the necessity for sustained public health interventions. Ultimately, this study emphasizes the importance of data-driven approaches for understanding and forecasting malaria trends, contributing to the goal of malaria eradication by 2030.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The transmission blocking activity of artemisinin-combination, non-artemisinin, and 8-aminoquinoline antimalarial therapies: a pooled analysis of individual participant data.\nAbstract: Background Interrupting human-to-mosquito transmission is important for malaria elimination strategies as it can reduce infection burden in communities and slow the spread of drug resistance. Antimalarial medications differ in their efficacy in clearing the transmission stages of Plasmodium falciparum (gametocytes) and in preventing mosquito infection. Here we present a combined analysis of six trials conducted at the same study site with highly consistent methodologies that allows for a direct comparison of the gametocytocidal and transmission-blocking activities of fifteen different antimalarial regimens or dosing schedules. Methods and findings Between January 2013 and January 2023, six clinical trials with transmission endpoints were conducted at the Clinical Research Centre of the Malaria Research and Training Centre of the University of Bamako in Mali. These trials tested Artemisinin-Combination Therapies (ACTs), non-ACT regimens and combinations with 8-aminoquinolines. Participants were males and non-pregnant females, between 5-50 years of age, who presented with P. falciparum mono-infection and gametocyte carriage by microscopy. Blood samples were taken before and after treatment for thick film microscopy, infectivity assessments by mosquito feeding assays and molecular quantification of gametocytes. Mixed-effects generalized linear models were fit with individual-specific random effects and fixed effects for time points, treatment groups and their interaction. Models quantified changes in mosquito infection rates and gametocyte densities within treatment arms over time and between treatments. In a pooled analysis of 422 participants, we observed substantial differences between ACTs in gametocytocidal and transmission-blocking activities, with artemether-lumefantrine (AL) being significantly more potent at reducing mosquito infection rates within 48 hours than dihydroartemisinin-piperaquine (DHA-PPQ), artesunate-amodiaquine (AS-AQ) and pyronaridine-artesunate (PY-AS) (p<0.0001). The addition of single low dose primaquine (SLD PQ) accelerated gametocyte clearance and led to a significantly greater reduction in mosquito infection rate within 48-hours of treatment for each ACT, while an SLD of the 8-aminoaquinoline tafenoquine (TQ) showed a delayed but effective response compared to SLD primaquine. Finally, our findings confirmed considerably higher post-treatment transmission after sulfadoxine-pyrimethamine plus amodiaquine (SP-AQ) compared to most ACTs, with a significantly lower relative reduction in mosquito infection rate at day 7 compared to DHA-PPQ, AS-AQ, and AL (p<0.0001). Therefore, adding an SLD PQ to SP-AQ may be beneficial to block malaria transmission in community treatment campaigns. Conclusions We found marked differences among ACTs and single low-dose 8-aminoquinoline drugs in their ability and speed to block transmission. The findings from this analysis can support treatment policy decisions for malaria elimination and be integrated into mathematical models to improve the accuracy of predictions regarding community transmission and the spread of drug resistance under varying treatment guidelines.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Burden of Latent Tuberculosis Infection among Healthcare Workers of Primary Health Centres and its Association with Clinical Laboratory Parameters\nAbstract: Objectives Healthcare workers (HCWs) are at high risk of latent tuberculosis infection (LTBI) due to their continued occupational exposure. The incidence of LTBI among the HCWs has seldom been investigated. Primary health centres (PHCs) provide effective and affordable medical care largely for the local population. The HCWs of PHCs who are likely to have increased occupational exposure have an increased risk of reactive as well as LTBI, and contribute to overall TB transmission than the general population. Methods A cross-sectional study (March-April 2024) was carried out to assess the burden of LTBI among the HCWs of PHCs (n=64) across Chennai, India. Blood samples were analyzed for gamma interferon using a QuantiFERON-TB Gold Plus assay. A complete baseline health profile, including complete blood count, blood glucose, liver and renal functions, and ferritin levels was estimated to identify potential biomarkers that are independently associated with LTBI. Results The present study revealed an LTBI prevalence of 25.3% (n=99) among PHC workers. The red cell distribution width was significantly associated with LTBI positivity among different biochemical and haematological parameters analyzed. Factors such as individuals age, underlying comorbid conditions (30.3%; n=30), and longer employment duration (28%; n=28) were significantly associated with IGRA positivity. IGRA positivity was significantly associated with decreased RDW-SD in females, overweight, and participants with O blood group. Conclusions The study reported a high prevalence of LTBI among HCWs of PHCs, which necessitates their periodical screening for the elimination of TB.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Effects of family planning counseling delivered during maternal healthcare on postpartum modern contraceptive uptake in low- and middle-income countries: a systematic review and meta-analysis\nAbstract: Background: Postpartum modern contraception is crucial for preventing unintended pregnancies, reducing short inter-pregnancy intervals, and improving maternal and child health outcomes. Family planning counseling, when engaging with maternal healthcare services, may enhance modern contraceptive uptake in the postpartum period. However, evidence in low- and middle-income countries (LMICs) has yielded conflicting findings. We aimed to explore the effects of family planning counseling delivered as part of maternal healthcare on postpartum modern contraceptive uptake in LMICs. Methods: The following six databases were searched in April 2024: PubMed, Web of Science, Embase, Global Health, Medline, and Scopus. Studies that examined the association between family planning counseling and modern contraception uptake in the postpartum period were included. The findings of the included studies were summarized narratively and through a random-effects meta-analysis when data supported. Trim and fill estimates and subgroup analyses were conducted to address publication bias and heterogeneity, respectively. Results: A total of 61 studies were included, of which 42 studies were considered for quantitative synthesis and the remaining 19 studies included in a narrative synthesis. We found that women who received family planning counseling during maternal healthcare visits were 2.75 times (95% CI, 2.11-3.58) more likely to use modern contraception during the postpartum period compared to those who did not receive family planning counseling. Although we observed significant publication bias and heterogeneity, addressing these issues through trim and fill estimation and subgroup analyses, respectively, produced estimates consistent with the summary findings. A narrative synthesis of an additional 19 studies also supports the effectiveness of family planning counseling during maternal healthcare visits on postpartum modern contraception uptake. Conclusion: Integrating family planning counseling into maternal healthcare can significantly increase modern contraceptive use in the postpartum period, as such effectively reducing unintended and short interval pregnancy as well as adverse maternal and child health outcomes. Policymakers should prioritize training healthcare providers and developing standardized protocols for effective counseling.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: SARS-CoV-2 Omicron XBB infections boost cross-variant neutralizing antibodies, potentially explaining the observed delay of the JN.1 wave in some Brazilian regions\nAbstract: The SARS-CoV-2 JN.1 lineage emerged in late 2023 and quickly replaced the XBB lineages, becoming the predominant Omicron variant worldwide in 2024. We estimate the epidemiological impact of this SARS-CoV-2 lineage replacement in Brazil and we further assessed the cross-reactive neutralizing antibody (NAb) responses in a cohort of convalescent Brazilian patients infected during 2023. We analyzed the evolution of SARS-CoV-2 lineages and severe acute respiratory infection (SARI) cases in Brazil between July 2023 and March 2024. We evaluated the cross-reactive NAb responses to the JN.1 variant in a cohort of convalescent Brazilian patients, both before and after infection with XBB.1* lineages. JN.1 replaced XBB with similar temporal dynamics across all country regions, though its epidemiological impact varied between locations. The Southeastern, Southern, and Central-Western regions experienced a brief XBB wave around October 2023, shortly before the introduction of JN.1, without any immediate upsurge of SARI cases during viral lineage replacement. By contrast, the Northeastern and Northern regions did not experience an XBB wave in the latter half of 2023 and displayed a rapid surge in SARI cases driven by the emergence of the JN.1. We found that recent XBB infections in the Brazilian population significantly boosted cross-reactive NAb levels against JN.1. The XBB wave observed in the second half of 2023 in some Brazilian states likely acted as a booster for population immunity, providing short-term protection against JN.1 infections and delaying the rise of SARI cases in certain regions of the country.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Gut colonization of Enterococcus species is associated with COVID-19 disease in Uganda\nAbstract: Background: Infection with the COVID-19-causing pathogen SARS-CoV-2 is associated with disruption in the human gut microbiome. The gut microbiome enables protection against diverse pathogens and exhibits dysbiosis during infectious and autoimmune disease. Studies based in the United States and China have found that severe COVID-19 cases have altered gut microbiome composition when compared to mild COVID-19 cases. We present the first study to investigate the gut microbiome composition of COVID-19 cases in a population from Sub-Saharan Africa. Given the impact of geography and cultural traditions on microbiome composition, it is important to investigate the microbiome globally and not draw broad conclusions from homogenous populations. Results: We used stool samples in a Ugandan biobank collected from COVID-19 cases during 2020-2022. We profiled the gut microbiomes of 114 symptomatic individuals who tested positive for SARS-CoV-2 along with 76 household contacts who did not present any symptoms of COVID-19. The inclusion of healthy controls enables us to generate hypotheses about bacterial strains potentially related to susceptibility to COVID-19 disease, which is highly heterogeneous. Comparison of the COVID-19 patients and their household contacts revealed decreased alpha diversity and blooms of Enterococcus and Eggerthella in COVID-19 cases. Conclusions: Our study finds that the microbiome of COVID-19 individuals is more likely to be disrupted, as indicated by decreased diversity and increased pathobiont levels. This is either a consequence of the disease or may indicate that certain microbiome states increase susceptibility to COVID-19 disease. Our findings enable comparison with cohorts previously published in the Global North, as well as support new hypotheses about the interaction between the gut microbiome and SARS-CoV-2 infection.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Design and evaluation of an automated real-time SOFA score calculator in an electronic health record system, for early sepsis diagnosis\nAbstract: Background: In patients with infections, the Sequential Organ Failure Assessment (SOFA) score should be used to diagnose sepsis. However, manual SOFA calculation is time consuming. Thus, we developed an automated real-time SOFA score application that has been fully implemented into the major electronic health record (EHR) system (TakeCare) in Stockholm. We aimed to describe the method and evaluate its performance for detection of sepsis. Method: We developed an automated SOFA application that presents a total score (SOFATotal) over time and a baseline score (SOFAChronic) based on outpatient data prior to hospital admission. We evaluated its performance on 583 sepsis alert patients in an emergency department, including 472 with sepsis according to manually calculated SOFA (SOFAManual). SOFATotal at 30 minutes and 3 hours, and quick-SOFA (qSOFA) were compared with SOFAManual for detection of sepsis. The acute SOFA score (SOFAAcute) was calculated by subtracting SOFAChronic from SOFATotal. Results: SOFATotal at 3 hours showed moderate-good agreement with SOFAManual (R2 = 0.76). Areas under receiver operating characteristic curves for sepsis detection were 0.83 at 30 minutes, 0.94 at 3 hours for SOFATotal, and 0.67 for qSOFA. Among patients with SOFAChronic [&ge;]1 (21% of cases), SOFATotal of [&ge;]2 was observed in 90% of cases (mortality 13.5%), although SOFAAcute of [&ge;]2 was noted in 63% (mortality 16.7%). Conclusion: The automated SOFA score demonstrated effectiveness in early sepsis detection, outperforming qSOFA, but adjustments for chronic baseline scores were necessary to avoid over-diagnosis. Further studies on broader patient populations would be useful to validate its clinical use.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Vaccine failure mode determines population-level impact of vaccination campaigns during epidemics.\nAbstract: Vaccines are a crucial tool for controlling infectious diseases, yet rarely offer perfect protection. \"Vaccine efficacy\" describes a population-level effect measured in clinical trials, but mathematical models used to evaluate the impact of vaccination campaigns require specifying how vaccines fail at the individual level, which is often impossible to measure. Does 90% efficacy imply perfect protection in 90% of people and no protection in 10% (\"all-or-nothing\"), or that the per-exposure risk is reduced by 90% in all vaccinated individuals (\"leaky\"), or somewhere in between? Here we systematically investigate the role of vaccine failure mode in controlling ongoing epidemics. We find that the difference in population-level impact between all-or-nothing and leaky vaccines can be substantial when R0 is higher, vaccines efficacy is intermediate, and vaccines slow but can't curtail an outbreak. Comparing COVID-19 pandemic phases, we show times when model predictions would have been most sensitive to assumptions about vaccine failure mode. When determining the optimal risk group to prioritize for limited vaccines, we find that modeling a leaky vaccine as all-or-nothing (or vice versa) can change the recommended target group. Overall, we conclude that models of vaccination campaigns should include uncertainty about vaccine failure mode in their design and interpretation.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Estimating the trend of COVID-19 in Norway by combining multiple surveillance indicators\nAbstract: Background: Estimating the trend of new infections was crucial for monitoring risk and for evaluating strategies and interventions during the COVID-19 pandemic. The pandemic revealed the utility of new data sources and highlighted challenges in interpreting surveillance indicators when changes in disease severity, testing practices or reporting occur. Aim: To estimate the trend in new COVID-19 infections by combining estimates of growth rates from all available surveillance indicators in Norway. Methods: We estimated growth rates from ten different surveillance indicators in Norway by using a negative binomial regression method and aligned the growth rates in time to hospital admissions by maximising correlations. Using a meta-analysis framework, we calculated overall growth rates and reproduction numbers, including assessments of the heterogeneity between indicators. Results: The estimated growth rates reached a maximum of 25% per day in March 2020, but afterwards they were between -10% and 10% per day. The correlations between the growth rates estimated from different indicators were between 0.5 and 1.0. Growth rates from indicators based on wastewater, panel and cohort data can give up to 14 days earlier signals of trends compared to hospital admissions, while indicators based on positive lab tests can give signals up to 7 days earlier. Conclusions: Combining estimates of growth rates from multiple surveillance indicators provides a good description of the COVID-19 pandemic in Norway. This is a powerful technique for a holistic understanding of the trends of new COVID-19 infections and the technique can easily be adapted to new data sources and situations.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Development, Testing, and Calibration of LINCS: A New Microsimulation Model of Maternal and Fetal Cytomegalovirus Infection\nAbstract: Background: Congenital cytomegalovirus (cCMV) is a leading cause of birth defects and the most common cause of non-genetic sensorineural hearing loss in children. There is a lack of decision modeling frameworks that can project cytomegalovirus (CMV) related patient outcomes and inform health policy. We created, tested, and calibrated a model of CMV acquisition and transmission in pregnancy using linked mother-infant dyads. Methods: We developed the Linking INfants and Mothers in Cytomegalovirus Simulation (LINCS) dyad-level Monte-Carlo microsimulation model of CMV infection among pregnant people and fetuses throughout pregnancy. We parameterized the model with data from the existing literature, implemented rigorous code testing procedures, and calibrated a key set of parameters to match model output to external data on cCMV prevalence and symptom risk. Results: A fully parameterized model for CMV among pregnant people and fetuses was developed, and the model code was confirmed to perform as specified. The calibration procedure identified parameter sets that generated model output closely matching the target values from the available data on cCMV prevalence and symptom risk. Conclusions: The LINCS model's ability to simulate the natural history of CMV infection during pregnancy was described and demonstrated, and the model was tested and calibrated to ensure proper functioning. Base case parameters were derived for CMV infection natural history to be used in future decision analyses of CMV testing and treatment strategies.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Longitudinal co-development of mental and cardio-metabolic health from childhood to young adulthood\nAbstract: Objective: Depressive symptoms and cardio-metabolic risk factors often co-occur. However, our understanding of the potential mechanisms and temporal dynamics underlying their co-development remains elusive. Method: This population-based cohort study examined bidirectional longitudinal associations between depressive symptoms and cardio-metabolic risk factors from age 10 to 25 years, using prospective data from the ALSPAC Study. Participants with at least one (of six) follow-up measurement for each outcome were included in the analyses. We measured depressive symptoms through self- as well as parent-reports, and assessed several cardio-metabolic risk factors (including adiposity measures, lipid profiles and inflammation). Results: Among our 7970 (47% male) participants, we found bidirectional, within-person associations between self-reported depressive symptoms and adiposity (i.e., fat/lean mass index, but not body mass index), across the study period. Adiposity was more stable over time ({beta} [range] = 0.75 [0.54; 0.84]), compared to depressive symptoms (0.26 [0.12; 0.38]), and it had a stronger prospective (i.e., cross-lagged) association with future depressive symptoms (0.07 [0.03, 0.13]) compared to that between depressive symptom and future adiposity (0.04 [0.03, 0.06]). The magnitude of these associations reached its peak between 14 and 16 years. We did not find evidence of cross-lagged associations in either direction between depressive symptoms and waist circumference, insulin, triglycerides, LDL cholesterol or C-reactive protein. Conclusions: These findings suggest a bidirectional relationship between depressive symptoms and cardio-metabolic risk factors, particularly adiposity (i.e., fat/lean mass). Adiposity showed a stronger prospective association with future depressive symptoms, than vice versa, however their relationship revealed more reciprocal than previously thought.\n"}, {"role": "assistant", "content": "yes"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Disrupted functional brain network associated with presence of hallucinations in Parkinsons Disease\nAbstract: Background Hallucinations negatively impact quality of life in Parkinsons disease, yet their neural mechanisms remain poorly understood, particularly in early disease stages. This study aimed to identify functional connectivity differences associated with visual hallucinations in early Parkinsons Disease and to validate these findings across independent datasets. Methods Resting state fMRI data from two prior independent studies was used (total N=185; N=84 hallucinators and N=101 non-hallucinators). Group differences in functional connectivity were assessed within predefined cytoarchitectonic cortical classes and functional networks, followed by whole-brain analysis using Network-Based Statistics (NBS). Associations with clinical measures, including hallucination severity, motor symptoms, cognition, and attention, were also evaluated. Results NBS identified a subnetwork of reduced functional connectivity in hallucinators, connecting regions involved in the default mode, somatomotor and attentional networks. This subnetwork was replicated in a matched sample from our independent cohort (N=50; p<0.01). Functional connectivity within the identified network was significantly associated with hallucination severity (Rsquare = 0.35, p = 0.01), and with baseline and future motor symptoms, cognition, and attention in hallucinators. Conclusions The identified functional subnetwork shows promise as a potential biomarker and therapeutic target for Parkinsons disease psychosis, warranting further investigation and validation in future studies.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Performance of rapid diagnostic tests, microscopy, and qPCR for detection of parasites among community members with or without symptoms of malaria in villages with high levels of artemisinin partial resistance in North-western Tanzania\nAbstract: Background: Despite the implementation of different control interventions, infections in the communities (among asymptomatic and symptomatic individuals) still play a crucial role in sustaining malaria transmission. This study evaluated the performance of rapid diagnostic tests (RDTs), microscopy, and quantitative PCR (qPCR) in detecting malaria parasites among community members in five villages of Kyerwa district, Kagera region, an area where artemisinin partial resistance (ART-R) has been recently confirmed. Methods: A community cross-sectional survey of asymptomatic and symptomatic participants (n=4454) aged [&ge;]6 months was conducted in July and August 2023. Malaria infections were detected using RDTs, microscopy, and qPCR (using the 18S RNA gene). Performance of RDTs and microscopy was assessed by sensitivity, specificity, and predictive values, using qPCR as the reference method. Factors affecting the accuracy of these methods were explored using a multivariate logistic regression model. Results: The prevalence of malaria infections was 44.4% (n = 1979/4454), 32.1% (n = 1431/4454), and 39.8% (n = 1771/4454) by RDTs, microscopy, and qPCR, respectively. The prevalence of P. malariae and P. ovale mono-infection by microscopy was 0.2% (n = 7/4454) and 0.3% (n = 12/4454), while by qPCR was 0.4% (n = 16/4454) and 0.5% (n = 24/4454), respectively. The geometric mean parasite densities (GMPDs) by microscopy were 642 (95% confidence intervals (CI) = 570 - 723), 126 (95% CI = 98 - 162), and 124 (95% CI = 82 - 160) asexual parasites/[micro]L; while by qPCR, the GMPDs were 1180 (95% CI = 1032 - 1349), 44 (95% CI = 32 - 61), and 50 (95% CI = 29 - 89) parasites/[micro]L for P. falciparum, P. ovale spp, and P. malariae, respectively. The sensitivity and specificity of RDTs were 94.0% (95% CI = 92.8% - 95.1%) and 87.5% (95% CI = 86.2% - 88.7%), respectively; whereas those of microscopy were 74.6% (95% CI = 72.5% - 76.6%) and 95.2% (95% CI = 94.3% - 96.0%), respectively. The sensitivity of RDTs and microscopy was low at very low parasitemia (<100 parasite/[micro]L), but increased significantly with increasing parasitemia, reaching [&ge;]99.6% at >10000 parasites/{micro}L (p<0.001). Conclusion: A higher prevalence of malaria was detected and the performance of RDTs and qPCR were comparable, but microscopy had a lower performance. Higher sensitivity by RDTs compared to microscopy, indicates that RDTs are effective for the detection of malaria infections for routine case management and surveillance of malaria in this area with confirmed ART-R; and can be utilized in the ongoing plans to develop a response to ART-R.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The Lifecycle of Electronic Health Record Data in HIV-Related Big Data Studies: A Qualitative Study of Instances of and Potential Opportunities to Minimize Bias\nAbstract: Background: Electronic health record (EHR) data are widely used in public health research, including HIV-related studies, but are limited by potential bias due to incomplete and inaccurate information, lack of generalizability, and lack of representativeness. This study explores how workflow processes within HIV clinics, among data scientists, and within state health departments may introduce and minimize bias within EHRs. Methods: Using a constructivist grounded theory approach, in-depth individual interviews were conducted with 16 participants purposively sampled in South Carolina from August 2023-April 2024. A focus group with 3 health department professionals with expertise in HIV disease surveillance was also conducted. Analysis was conducted as outlined by Charmaz (2006). Results: To reduce bias in EHR data, information entry forms should be designed to expansively include patient self-reported social determinants of health (SDOH) information. During data collection, healthcare providers should create a supportive healthcare environment, facilitate SDOH information disclosure, and accurately document patient information. Patients should have access to their EHRs to confirm that SDOH information are correctly recorded. During data curation, data scientists should inspect datasets for completeness, accuracy, and educate public health researchers on dataset limitations. During data management and utilization, health department professionals should crossmatch data across the state, customize data collection systems to reflect local needs, and provide community-based data education and stigma management. Conclusion: Study results suggest that future research is needed to understand how healthcare systems can be incentivized to create and implement EHR bias reduction strategies across all workflows and between stakeholders.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Sustained attention processes in very preterm adolescents and their relationship to socio-emotional competence.\nAbstract: Very preterm (VPT) adolescents are at high risk of impaired sustained attention processes, as well as behavioral and socio-emotional problems. Previous studies have highlighted altered attentional patterns of brain activation in this population, but results are inconsistent. The current study aims to explore brain activity related to sustained attention in VPT and full-term adolescents aged 11 18, as well as its associations with attentional capacities and socio emotional competences. Event related functional MRI (fMRI) was used to assess sustained attention performance and associated brain activations by comparing VPT (n = 34) and their age matched full term (FT, n = 28) peers from a previously validated continuous performance task with gradual onset (gradCPT) paradigm, using two different modality versions (i.e., face and scene). In both groups, linear regression analyses were performed to examine associations between attentional and socio emotional difficulties and brain activations related to sustained attention. Results show preserved sustained attention processes in VPT adolescents, indicated by comparable behavioral attentional performance and cerebral patterns of activations in both groups across the two modalities of the gradCPT. In addition, VPT adolescents showed over recruitments in posterior occipital areas compared to FT adolescents. Moreover, higher socio emotional difficulties (i.e., higher anxiety and social difficulties) in VPT were linked to altered activations specifically in the right middle frontal gyrus, occipito temporal gyri and bilateral cerebellum, but exclusively observed during the face modality of the gradCPT. Overall, these results suggest that despite preserved sustained attention competences, VPT adolescents present a less mature sustained attention cerebral network, particularly during a task with a social context.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Investigation of the urinary peptidome to unravel collagen degradation in health and kidney disease\nAbstract: Naturally occurring fragments of collagen type I alpha 1 chain (COL1A1) have been previously associated with chronic kidney disease (CKD), with some fragments showing positive and others negative associations. Using urinary peptidome data from healthy individuals (n=1131) and CKD patients (n=5585) this aspect was investigated in detail. Based on the hypothesis that many collagen peptides are derived not from the full, mature collagen molecule, but from (larger) collagen degradation products, relationships between COL1A1 peptides containing identical sequences were investigated, with the smaller (offspring) peptide being a possible degradation product of the larger (parent) one. The strongest correlations were found for relationships where the parent differed by a maximum of 3 amino acids from the offspring, indicating an exopeptidase-regulated stepwise degradation process. Regression analysis indicated that CKD affects this degradation process. Comparison of matched CKD patients and control individuals (n=612 each) showed that peptides at the start of the degradation process were consistently downregulated in CKD, indicating an attenuation of COL1A1 endopeptidase-mediated degradation. However, as these peptides undergo further degradation, likely mediated by exopeptidases, this downregulation can become less significant or even reverse, leading to an upregulation of later stage fragments and potentially explaining the inconsistencies observed in previous studies.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Identification of novel nutrient-sensitive human yolk sac functions required for embryogenesis\nAbstract: The human yolk sac (hYS) is essential for embryo nutrient biosynthesis/transport and development. However, there lacks a comprehensive study of hYS nutrient-gene interactions. Here we performed a secondary analysis of hYS transcript profiles (n=9 samples) to identify nutrient-sensitive hYS genes and regulatory networks, including those that associate with adverse perinatal phenotypes with embryonic origins. Overall, 14.8% highly expressed hYS genes are nutrient-sensitive; the most common nutrient cofactors for hYS genes are metals and B-vitamins. Functional analysis of highly expressed hYS genes reveals that nutrient-sensitive hYS genes are more likely to be involved in metabolic functions than hYS genes that are not nutrient-sensitive. Through nutrient-sensitive gene network analysis, we find that four nutrient-sensitive transcription regulators in the hYS (with zinc and/or magnesium cofactors) are predicted to collectively regulate 30.9% of highly expressed hYS genes. Lastly, we identify 117 nutrient-sensitive hYS genes that associate with an adverse perinatal outcome with embryonic origins. Among these, the greatest number of nutrient-sensitive hYS genes are linked to congenital heart defects (n=54 genes), followed by microcephaly (n=37). Collectively, our study characterises nutrient-sensitive hYS functions and improves understanding of the ways in which nutrient-gene interactions in the hYS may influence both typical and pathological development.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Adverse respiratory events during treatment with gabapentin and opioids among older adults with spine-related conditions: a propensity-matched cohort study in the US Medicare population\nAbstract: Background Context: Recent work indicates no increased mortality risk with concurrent gabapentin and opioid use when using an active comparator control design. However, concurrent gabapentin and opioid prescriptions have been associated with greater risk of respiratory depression in some studies. Purpose: To compare the risk of respiratory events among Medicare enrollees with spine-related diagnoses treated with gabapentin + opioids vs those treated with tricyclic antidepressants (TCA) or duloxetine + opioids. We hypothesized that enrollees treated with gabapentin + opioids would have increased risk of adverse respiratory events compared to those treated with an active control + opioids. Study Design/Setting: Propensity score-matched cohort study with an incident user, active comparator (TCA/duloxetine) control design. The primary analysis included those who concurrently (within 30 days) filled [&ge;]1 incident gabapentin + [&ge;]1 opioid or [&ge;]1 incident TCA/duloxetine + [&ge;]1 opioid prescription. Patient Sample: U.S. Medicare beneficiaries with spine-related diagnoses 2017-2019. People treated with gabapentin + opioids (n=66,580) were matched on demographic and clinical factors to people treated with TCAs/duloxetine + opioids (n=66,580). Outcome Measures: Time to a composite respiratory outcome consisting of mechanical ventilation, intubation, respiratory failure, pneumonia, or acute respiratory distress syndrome. Methods: Cox proportional hazard regression was used to estimate adjusted hazard ratios (aHRs) and 95% confidence intervals (95% CIs). Results: Among 133,160 Medicare enrollees (median age 73.3 years; 66.7% female), 6089 (4.6%) experienced respiratory events before the end of follow-up. A total of 3297 (5.0%) of people who were treated with gabapentin + opioids (median initial dose/day of gabapentin was 338 mg) had respiratory events compared to 2792 (4.2%) of those treated with an active control + opioids. The increased risk in those treated with gabapentin + opioids was statistically significant after adjustment (HR 1.14; 95% CI 1.09, 1.20; p<0.0001). The most common respiratory events were pneumonia (3.5% of people in the gabapentin + opioids group versus 3.0% of people in the TCA/duloxetine + opioids group) and respiratory failure (2.2% in the gabapentin + opioids group versus 1.8% in the TCA/duloxetine + opioids group). Results were similar in analyses (a) restricted to [&le;]30-day follow-up and (b) that required [&ge;]2 fills of each prescription. Conclusions: While recent work has indicated no increased mortality risk with concurrent gabapentin and opioid use, the current findings suggest clinicians should exercise caution in prescribing gabapentin to people experiencing pain who are also being treated with opioids, due to the potentially increased risk of respiratory events.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The neurodevelopmental spectrum of CASK-related disorder\nAbstract: Background: Pathogenic CASK variants are associated with neurodevelopmental disorders of variable severity including X-linked intellectual disability (XLID) and microcephaly with pontocerebellar hypoplasia (MICPCH). Although the number of diagnosed cases is rising, current understanding of the CASK-related neurodevelopmental spectrum is limited. Here, we systematically review the published characteristics of individuals with CASK-related disorder, and compare these to a more recently-diagnosed group. We provide quantitative information about the ranges of adaptive abilities, motor function, visual function and social-emotional-behavioural characteristics, and explore within-group associations. Methods: 151 individuals with CASK variants were identified in published literature. 31 children and young people with CASK variants were recruited to the UK-based Brain and Behaviour in Neurodevelopmental disorder of Genetic Origin (BINGO) project. BINGO-participating caregivers completed a bespoke medical history questionnaire and battery of standardised neurodevelopmental measures. Results: Comparing the recently diagnosed BINGO CASK-related disorder group to previously reported individuals, we found consistent prevalence of tone abnormalities, sensorineural hearing loss and epilepsy, but lower prevalence of severe/profound ID, MICPCH, optic atrophy and nystagmus. Areas of frequent difficulty not highlighted in previous reports include sleep difficulties and cerebral visual impairment (CVI). Neurodevelopmental characteristics were highly variable within the BINGO CASK-related disorder group, and group-wide patterns were similar to those observed in other rare genetic conditions. Within the BINGO CASK-related group, epilepsy is significantly associated with ID severity, after controlling for age. Sub-groups with MICPCH or microcephaly only have equivalent ranges of adaptive function, but MICPCH may be associated with more severe motor difficulties. Conclusion: The spectrum of neurodevelopmental characteristics associated with CASK-related disorder appears to be broadening with increased access to genome-wide diagnostic testing. Further studies are needed to elucidate the relationships between CASK variants, structural brain development, epilepsy, and neurodevelopmental characteristics.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Long-Read Sequencing Increases Diagnostic Yield for Pediatric Sensorineural Hearing Loss\nAbstract: The diagnostic yield of genetic testing for pediatric sensorineural hearing loss (SNHL) has remained at around 40% for over a decade despite newly discovered causative genes and the expanded use of exome sequencing (ES). This stagnation may be due to (1) a focus on coding regions of the genome and (2) an inability to resolve variants in complex genomic regions due to reliance on short-read sequencing technologies. Short-read genome sequencing (srGS) and long-read genome sequencing (lrGS) both provide exonic single nucleotide variant (SNV) and small indel detection at the same sensitivity as ES, but also evaluate intronic regions. lrGS provides improved resolution for structural variants (SV) and repetitive genomic regions. We sought to investigate the potential utility of lrGS in the diagnostic evaluation of a small cohort of patients with SNHL of unknown etiology after ES and srGS. 19 pediatric patients with SNHL underwent lrGS via PacBio SMRT sequencing. Sequencing data were processed using the PacBio WGS variant pipeline. The diagnostic yield for this lrGS cohort was 4/19 (21%). Relevant variants detected only with lrGS included a hemizygous deletion in trans with a missense variant in an area of high genomic homology (OTOA) and two single nucleotide loss-of-function variants in trans to a known copy-number-loss for a gene with a highly homologous pseudogene (STRC). A complex inversion was identified in the MITF gene which was also identified on post-hoc analysis by srGS. LrGS provides improved resolution for complex genomic structural variation which may increase diagnostic yield for genetic pediatric SNHL, and, potentially, rare disease more broadly.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The contribution of coding variants to the heritability of multiple cancer types using UK Biobank whole-exome sequencing data\nAbstract: Genome-wide association studies have been highly successful at identifying common variants associated with cancer, however, they do not explain all the inherited risk of cancer. Family-based studies, targeted sequencing and, more recently, exome-wide association studies have identified rare coding variants in some genes associated with cancer risk, but the overall contribution of these variants to the heritability of cancer is less clear. Here we describe a method to estimate the genome-wide contribution of rare coding variants to heritability that fits models to the burden effect sizes using an empirical Bayesian approach. We apply this method to the burden of protein-truncating variants in over 15,000 genes for 11 cancers in UK Biobank, using whole-exome sequencing data on over 400,000 individuals. We extend the method to consider the overlap of genes contributing to pairs of cancers. We found ovarian cancer to have the greatest proportion of heritability attributable to protein-truncating variants in genes (46%). The joint cancer models highlight significant clustering of cancer types, including a near complete overlap in susceptibility genes for breast, ovarian, prostate and pancreatic cancer. Our results provide insights into the contribution of rare coding variants to the heritability of cancer and identify additional genes with strong evidence of susceptibility to multiple cancer types.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Pretrained Patient Trajectories for Adverse Drug Event Prediction Using Common Data Model-based Electronic Health Records\nAbstract: Pretraining electronic health record (EHR) data through language models by treating patient trajectories as natural language sentences has improved various medical tasks. However, EHR pretraining models have never been utilized in adverse drug event (ADE) prediction. Here, we propose a novel pretraining scheme for common data model (CDM) based EHR data, named CDM-BERT. We utilized diagnosis, prescription, measurement, and procedure domains from observational medical outcomes partnership (OMOP)-CDM. We newly adopted domain embedding (DE) to simplify pretraining procedure and to improve comprehension of medical context. ADE prediction was selected as a finetuning task. For drug groups, we included nonsteroidal anti-inflammatory drugs (NSAID), anticoagulants (AC), glucocorticoids (GC), and chemotherapy (Chemo). For corresponding adverse events, we selected peptic ulcer (PU), intracranial hemorrhage (ICH), osteoporosis (OP), and neutropenic fever (NF), respectively. CDM-BERT was validated by internal and external datasets with 510,879 and 419,505 adult inpatients. CDM-BERT outperformed all the other baselines in all cohorts, demonstrating the effectiveness of DE (area under the receiver operating characteristic curve (AUROC) of 0.977, 0.908, 0.980, 0.989 for NSAID-PU, AC-ICH, GC-OP, Chemo-NF cohorts in internal validation, and 0.967, 0.960, 0.972, 0.959 in external validation, respectively). We also identified important features for each cohort, and several prior studies and clinical knowledge suggested the results. CDM-BERT has demonstrated its potential as a foundation model through its prediction performance, interpretability, and compatibility.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: I answer to clinical scenarios as a real doctor because I \"Think\": \"ChatGPTo1\" new reasoning ability.\nAbstract: The adoption of the ChatGPT o1 model represents a significant advancement in the management of clinical cases due to the introduction of a new structured reasoning capability, the \"chain-of-thought reasoning\" (CoT). In this study, 350 general medicine clinical cases were tested using ChatGPT o1 and ChatGPT o1 mini, and their performance was compared with ChatGPT 4o and ChatGPT 4o mini to evaluate diagnostic accuracy. The results showed that ChatGPT o1 achieved a correct answer rate of 93.4%, outperforming both ChatGPT 4o (82.2%) and the mini versions (ChatGPT o1 mini: 70.2% and ChatGPT 4o mini: 66.2%). The CoT technique enabled the model to provide more coherent and transparent responses, reducing the occurrence of so-called \"hallucinations.\" This study highlights how the ChatGPT o1 model can be a valuable tool in clinical practice, although its use requires supervision to ensure patient safety, especially in critical settings.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Machine learning framework for predicting the presence of high-risk clonal haematopoiesis using complete blood count data: a population-based study of 431,531 UK Biobank participants\nAbstract: Background Clonal haematopoiesis (CH), the disproportionate expansion of a haematopoietic stem cell and its progeny, driven by somatic DNA mutations, is a common age-related phenomenon that engenders an increased risk of developing myeloid neoplasms (MN). At present, CH is identified by targeted sequencing of peripheral blood DNA, which is impractical to apply at population scale. The complete blood count (CBC) is an inexpensive, widely used clinical test. Here, we explore whether machine learning (ML) approaches applied to CBC data could predict individuals likely to harbour CH and prioritise them for DNA sequencing. Methods The UK Biobank was filtered to identify 431,531 participants with paired CBC and whole exome sequencing (WES). Somatic mutations were previously identified from blood WES using Mutect2 to classify individuals with CH driver mutations. Using 18 CBC indices/features and basic demographics (age and sex), we trained a range of tree-based ML classifiers to infer as binary output, the presence/ absence of CH. Findings Using Random Forest (RF) classifiers, we predicted the presence/absence of CH driven by mutations in one of five genes known to confer a high-risk of incident MN (JAK2, CALR, SF3B1, SRSF2 and U2AF1). We subsequently developed a unified, optimised RF classifier for high-risk CH driven by any of these genes and assessed its performance (median AUC 0.85). However, the low prevalence of high-risk CH implies that our model cannot be generalised to population scale without compromising its sensitivity (20.1% using stringent cutoff probability score). Interpretation We showcase a proof-of-concept that the presence of high-risk CH can be inferred from CBC perturbations using RF classifiers. The future integration of raw blood cell analyser data can help improve the performance of our model and facilitate its application at scale.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Risk of cardiovascular disease in patients with classical Hodgkin lymphoma: a Danish nationwide register-based cohort study\nAbstract: The risk of cardiovascular disease (CVD) in patients with classical Hodgkin lymphoma (cHL) undergoing contemporary treatment is unclear. Methods: All patients with cHL [&ge;]18 years at diagnosis treated with doxorubicin-containing chemotherapy between 2000-2022 were matched with comparators in a 1:5 ratio on birth year, sex, and Charlson Comorbidity Index at time of matching (score of 0 or [&ge;]1). The cause-specific cumulative incidence of a composite of CVDs and corresponding 95% confidence intervals (CIs) were computed with death and lymphoma relapse as competing events using the Aahlen-Johansen estimator. Results: A total of 1,905 patients and 9,525 comparators with a median follow-up of 10 years (interquartile range, [IQR]: 5.9-17.4). Median age was 39 years (IQR: 27-56), median cumulative doxorubicin dose was 250 mg/m2 (IQR: 200-300). The CVD cumulative incidences were 4.7% (95% CI: 3.6-5.7) for patients vs. 2.6% (95% CI: 2.3-2.9) for comparators at 5 years, 8.9% (95% CI: 7.2-10.5) vs. 5.5% (95% CI: 4.9-6.0) at 10 years, and 17.0% (95% CI: 14.1-19.9) vs. 8.2% (95% CI: 7.4-9.0) at 15 years. Conclusions: CVD remains a substantial effect after contemporary treatment for cHL, suggesting that awareness of symptoms and a low threshold for referral to diagnostic examination are still important measures during survivorship.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Acceptability and implementation considerations of the patient-collected rectal swab for sexually transmitted infections testing among men who have sex with men and transgender women in Kigali, Rwanda\nAbstract: Background Rectal sexually transmitted infections (STI) are prevalent among men who have sex with men (MSM) and transgender women (TGW). Self-collection of rectal specimens is widely used globally, but limited data exist on its implementation in Africa. We report experiences of MSM/TGW self-collecting rectal STI specimens in Kigali. Methods From March to August 2018, 738 MSM/TGW were recruited in a cross-sectional study using respondent-driven sampling in Kigali. We tested for Neisseria gonorrhea and Chlamydia trachomatis using the Cepheid GeneXpert CT/NG platform on self-collected rectal swabs. Likert scales assessed self-collection difficulty and comfort. Ordered logistic regression analyses were performed to characterize factors associated with self-collection difficulty. Results Overall, 14%(106) identified as TGW. In total, 78%(577) found rectal swab self-collection easy/very easy, while 15%(108) found it difficult/very difficult. Most, 92%(679), were comfortable/very comfortable with the test, and 98%(730) said they would repeat the test in the future. In multivariable RDS-adjusted analyses, discomfort with the swab was positively associated with difficulty in self-collection (adjusted cumulative odds ratios ((aCOR): 7.9(95%CI: 4.9-12.8)) and having a prevalent rectal STI (aCOR: 4.19, 95%CI: 2.02-8.72) was significantly associated with difficulty. Furthermore, 10%(75) of rectal swabs returned indeterminate results (65 invalid results and 10 errors). Conclusion Most MSM/TGW found self-collection of rectal swabs easy, comfortable, and collected valid samples. Self-collection of rectal swabs could be used alongside clinic- and community-based STI testing to improve diagnosis and treatment in Rwanda. However, measures to optimize sample collection and processing are needed to reduce the cost and clinical implications of indeterminate results\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Altered association of plasmatic oxytocin with affective stress responses in alcohol use disorder\nAbstract: Oxytocin has been proposed to play a role in the development and maintenance of alcohol use disorder (AUD) through its interactions with stress pathways. Empirical evidence that indicates altered associations between endogenous oxytocin and stress reactivity in AUD is currently lacking. In this study, we investigated baseline plasmatic oxytocin concentrations of early-abstinent patients with AUD (N = 40) and matched healthy control participants (N = 37), who completed the Trier Social Stress Test (TSST) as well as a control task on two separate visits. We measured salivary cortisol and pulse rate as indicators of a physiological stress response, and anxiety ratings as an indicator of an affective stress response at multiple time points. Baseline oxytocin levels did not significantly differ between the groups. However, our results suggest an altered association of oxytocin and affective stress responses in participants with AUD: while participants with AUD showed a positive relationship between plasmatic oxytocin levels and stress-induced anxiety increase, the opposite relationship was found in control participants. We did not find evidence for an association of oxytocin with physiological stress responses. Assuming that affective stress responses mediate addiction-related behaviors like craving and relapse, our findings suggest that administering exogenous oxytocin might not be beneficial in treating AUD, at least during the early stages of abstinence.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Bridging Cultures to Defeat COVID-19: An Innovative Virtual Exchange Program in Global Medical Education\nAbstract: The problem and opportunity: There is a critical and growing need to train globally focused, culturally fluent clinicians and scientists who can collaboratively defeat current and future public health threats across international boundaries. In parallel, academic conferences bring together thousands of diverse international healthcare professionals every year, yet their potential to provide the crucial professional development training necessary to advance internationalized medicine is often underutilized. The solution: We developed and now first report an innovative healthcare education program that used an academic conference as the framework around which to build a structured, non-incidental virtual exchange (VE) for training globally and culturally proficient healthcare professionals. Herein we further describe the program's design and content, successes and challenges, and lessons learned. Program Overview: Using a smartphone based social-networking and conference management app with available translation capabilities, pre- and post-graduate trainees prepared and participated in poster presentations, seminars, and workshops to learn current research and best-practices in COVID-19 medicine, while engaging with their international peers in networking and professional-development exercises. The 2-week intensive program included daily synchronous interactive seminars on various topics in COVID-19 medicine, international team-based asynchronous activities such as preparing, presenting, and constructively critiquing research posters at virtual poster sessions, and expert-led wellness and cultural-competence workshops. Participants received initial training in the norms of intercultural communication, syllabus content and expectations, incentives, icebreaker activities, and program technology. They learned then-current COVID-19 medical research, therapies, and best practices, as well as professional \"soft skills\" including leadership, team building, scientific/clinical presentation, verbal/written communication skills, and intercultural competence. The program vastly expanded participants' international professional networks to enhance their mentorship and career development opportunities. Conclusions: Participants reported receiving substantial benefits from the program, with many reporting immediate translation of lessons learned toward improving healthcare education or practice in their home communities.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Impaired speaking-induced suppression in schizophrenia predicts auditory verbal hallucinations\nAbstract: A successful efference copy self-prediction suppresses auditory signals in the primary auditory cortex (A1) is necessary for speakers to successfully compare auditory feedback during speech production with auditory feedback during passive listening, this is called speaker-induced suppression (SIS). The top-rank positive symptom in schizophrenic (SZ) patients, auditory verbal hallucination, for instance, is hypothesized to relate to failure to distinguish the internal voice and external sounds, and this deficit is thought to be associated with impaired self-prediction in comparing external and self-generated contents. In this magnetoencephalographic imaging (MEGI) study, we compared SIS M100 in the primary auditory cortex (A1) between the healthy controls (HC; N = 30) and SZ patients (N = 22). The SZ patients displayed reduced SIS and M100 in the A1, and this impairment is negatively correlated with auditory hallucinations. These outcomes suggest that the SZ patients' hallucinatory symptoms are caused by misattribution between the external and self-generated stimuli. We proposed that the weakened self-agency and neural oscillations may lead to this misattribution.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Reversible and Noninvasive Modulation of a Historical Surgical Target for Depression with Low Intensity Focused Ultrasound\nAbstract: Major depressive disorder has a point prevalence of 5% of the world population and is the main cause of disability, with up to a third of patients not responding to first-line treatments. Surgical neuromodulation is offered to only an anecdotal proportion of these patients, because while these methods are curative in some individuals, the proportion of responders rarely exceeds 50%. Recent efforts to establish reliable brain circuit-symptom relationships and thus predict response have involved mapping with multiple intracranial electrodes, but the impracticality of this approach currently prevents its application at scale. In the present study (ClinicalTrials.gov identifier NCT05697172; FDA Q220192) we begin to address this gap by leveraging low-intensity focused ultrasound (LIFU), a novel noninvasive technique, to modulate the anterior limb of the internal capsule, which is an established surgical deep white matter target for depression. We based our study on burgeoning in vitro evidence that LIFU attenuates axonal conduction by operating mechanosensitive channels in nodes of Ranvier. Compared with sham stimulus, active LIFU produced a functional disconnection of gray matter hubs reached by the sonicated axonal tracts, an increase in positive emotion, and top-down effects on the cardiovascular autonomic balance. Our results using LIFU of deep-brain white matter tracts in humans open three potential avenues to understand the mechanisms and improve the outcome of depression, namely attaining a personalized definition of brain circuit-symptom relationships, serving as a noninvasive probe for neuromodulation before irreversible procedures in a \"try before you buy\" approach, and ultimately emerging as a therapeutic intervention itself.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Non-invasive Real-time Detection of Potassium level Changes in Skeletal Muscles during Exercise by Magnetic Resonance Spectroscopy\nAbstract: Introduction: Potassium is essential in cellular functions, with specific importance in muscle activity and cardiovascular health. It is the main intracellular cation in the human body with 70% located in muscle. Traditional methods to measure potassium levels are invasive and lack specificity for intracellular concentrations. Recently, non-invasive in vivo investigation of K+ ion homeostasis has become feasible by using 39K Magnetic Resonance Imaging (MRI) and MR spectroscopy (MRS) at ultrahigh magnetic fields. However, studies demonstrating the sensitivity of 39K MRI or MRS to detect potassium alterations in disease or upon intervention are sparse. This study utilizes 39K MRS to non-invasively track real-time intramuscular potassium changes during exercise, providing an assessment of potassium dynamics and explores the potential for technical artifacts in the measurements. Methods: Five healthy subjects (three males, two females) were recruited to perform standardized dynamic knee extensions inside a 7T MR scanner. Potassium levels were measured using a 39K MRS protocol that included periods of rest, moderate, and heavy exercise followed by recovery. Additionally, possible measurement artifacts due to muscle movement or changes in coil position relative to the thigh were evaluated using 39K MRS and 1H MRI monitoring in separate sessions. Results: The study revealed a consistent decrease in potassium levels during both moderate and heavy exercise, with an average decrease of 5-6%. These changes were rapidly detectable and were reversed upon cessation of exercise, indicating effective in vivo monitoring capability. Possible experimental artifacts were investigated, and the results suggested not to be responsible for the detected potassium changes during exercise. The results of the non-localized 39K MRS measurements during exercise correlated well with expected physiological changes based on previous literature. Discussion: The application of 39K MRS provides a valuable non-invasive tool for studying potassium dynamics in human skeletal muscle. This technique could enhance our understanding of muscle physiology and metabolic disorders. The ability to measure these changes in real time and non-invasively highlights the potential for clinical applications, including monitoring of diseases affecting muscle and cellular metabolism.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Beta-blockers Increase Cardiovascular Mortality in Hemodialysis Patients with Orthostatic Hypotension\nAbstract: Background: Orthostatic hypotension (OH) is prevalent among dialysis patients and is a known cardiovascular (CV) risk factor. Beta-blockers (BBs) are commonly prescribed to mitigate CV mortality in this population despite potential risks associated with OH. This study examines the impact of BBs on CV mortality among dialysis patients with OH. Methods: We conducted a prospective analysis of 134 dialysis patients from the PRECADIA program at the Centro de Educacion Medica e Investigaciones Clinicas (CEMIC) focusing on hemodynamic assessments including blood pressure changes from supine to standing positions and evaluating CV mortality over a 3-year follow-up. OH was defined by a decrease of 10 mmHg or more in diastolic blood pressure or 20 mmHg or more in systolic blood pressure upon standing. Cox regression analyses were utilized to identify independent predictors of CV mortality. Results: Of the patients, 23.1% were identified with OH. No significant differences in demographic or baseline clinical characteristics were observed between patients with and without OH, except for a higher diabetes prevalence in the OH group. OH patients treated with BBs demonstrated significantly higher CV mortality (29.6%) compared to those not receiving BBs. Independent predictors of CV mortality included age, time on dialysis, and BB use, with BBs significantly associated with increased CV mortality risk. Conclusions: Dialysis patients with OH exhibit a high CV mortality rate significantly influenced by BB usage. While BBs are recommended for managing CV risks in dialysis patients, their association with increased CV mortality in patients with OH necessitates careful consideration and management strategies.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Measuring Multi-Joint Upper Limb Proprioceptive Position Sense in Children with Unilateral Spastic Cerebral Palsy using 3D Kinematics\nAbstract: Background: Proprioceptive position sense (PPS), which provides the awareness of the positions of one's limbs in space, is compromised in children with unilateral spastic cerebral palsy (USCP). Relationships between decreased PPS, skill acquisition, and performance are not well understood partly due to differences between measurement modalities of PPS. During assessments, individual joints are often constrained to a single plane of movement. We introduce ways to estimate PPS, simultaneously from multiple upper limb joints during unconstrained pose matching tasks, using 3D kinematics. Methods: Nineteen children with USCP, 14 age-matched typically developing children (TDC) and, 16 typically developed adults (TDA) were recruited. Participants performed contralateral upper limb pose matching tasks while blindfolded. Using 3D kinematic data extracted by a VICON Nexus motion-capture system, three ways of measuring matching performance were calculated: joint angle symmetry, joint distance symmetry, and orientation symmetry. Mann-Whitney U tests were used to statistically compare scores between groups. Spearman's rank correlation was used to investigate relationships between PPS symmetry measures and hand function in children with USCP. Results: Children with USCP presented significantly lower angle, distance, and orientation symmetries compared to TDC. TDA scores were not statistically significantly different compared to TDC. In children with USCP, orientation symmetry measures strongly correlated with several clinical hand function scores such as the box and blocks test. Conclusions:We introduced a new way to conveniently measure multi-joint PPS. Children with USCP presented lower PPS estimates compared to TDC. We also observed that orientation symmetry (limb coordination) strongly correlates with hand function.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Feasibility of an App-Assisted and Home-Based Video Version of the Timed up and Go Test for Patients with Parkinson Disease: vTUG\nAbstract: Background: Parkinson Disease (PD) is a progressive neurological disorder. Current therapeutic trials investigate treatments that can potentially modify the disease course. Testing their efficiency requires frequent and precise clinical outcome assessments (COA) of symptoms that remain problematic under symptomatic treatments, such as gait and balance. Home-based examinations may enhance patient compliance and, in addition, produce more reliable results by assessing patients more regularly in their familiar surroundings. Objective: The objective of this study was to assess the feasibility of a digitized COA designed to video record the Timed up and Go (vTUG) test at home via a study-specific smartphone app for patients with PD. Method: In this study, 28 patients were recruited and asked to perform at home each week a set of three consecutive vTUG tests over a period of 12 weeks using an app. The videos were subjected to a manual review to ascertain the durations of the individual vTUG phases, as well as to identify any errors or deviations in the setup that might have influenced the result. To evaluate the usability and user-friendliness of the vTUG and app, the System Usability Scale (SUS) and User Experience Questionnaire (UEQ) were administered to patients at the study end. Results: Overall, 19 patients completed the 12-week study, 17 of which recorded 10 videos or more. A total of 706 vTUGs with complete timings were recorded. Random Forest Regression yielded time to walk up as the most important segment of the vTUG for predicting the total time. Variance of vTUG total time was significantly higher between weeks than it was between the three consecutive vTUGs at one time point [F(254,23) = 6.50, p < 0.001]. The correlation between vTUG total time and UPDRS III total score was weak (r = 0.24). Yet, correlation between vTUG and a derived gait subscore (UPDRS III items 9-13) was strong (r = .59). A linear mixed-effects model revealed a significant effect of patient-reported motion status on vTUG total time. Including additional variables such as UPDRS III gait subscore, different footwear, and chairs used, further improved the model fit. Conclusion: Assessment of gait and balance by home-based vTUG is feasible. Factors influencing the read-out were identified and could be controlled for future use and longitudinal trials.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: A Large Scale Multi-dataset Investigation of Brain Metastases Distribution Based on Primary Cancer Type\nAbstract: Brain metastases (BM) present a significant clinical challenge, characterized by poor prognosis and high morbidity. Previous research has demonstrated heterogeneous distribution patterns of BMs depending on the primary tumor type. This study aims to further investigate the distribution patterns of BMs. Utilizing three public datasets and one private dataset, we analyzed 1,344 patients with a total of 9,431 identified BMs, categorizing them into lung, breast, melanoma, kidney cancer, and other groups. Our findings indicate that lung cancer BMs predominantly localize in the cerebellum, as well as the frontal, temporal, occipital, and parietal lobes. Breast cancer BMs are commonly found in the cerebellum, frontal, occipital, and temporal lobes, while melanoma BMs are most frequently located in the occipital, temporal, and frontal lobes. Kidney cancer BMs, less studied in existing literature, primarily affect the frontal and temporal lobes. Additionally, we explored the relationship between BM volume and tumor type, revealing that lung and breast cancer BMs are more likely to have smaller tumor volumes compared to melanoma and kidney cancer BMs. These findings confirm and expand upon previous research, offering new insights into the distribution and characteristics of kidney cancer BMs.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Artificial Intelligence Applied to Electrocardiographic Images for Scalable Screening of Transthyretin Amyloid Cardiomyopathy\nAbstract: Background: Transthyretin amyloid cardiomyopathy (ATTR-CM) remains largely under-recognized, under-diagnosed, and under-treated. We hypothesized that the myocardial remodeling of ATTR-CM may be detectable through artificial intelligence (AI) applied to 12-lead electrocardiographic (ECG) images. Methods: Across 5 hospitals of a large U.S.-based hospital system, we identified patients with ATTR-CM, defined by the presence of a positive nuclear scan with an approved bone radiotracer or pharmacotherapy with an approved transthyretin stabilizer between 2015 and the first half of 2023. The development cohort consisted of 1,011 ECGs from 234 patients (age 79 [IQR:70-85] years, n=176 [17.4%] women), who were age- and sex-matched in a 10:1 ratio to 10,110 ECGs from 10,110 controls (age 79 [IQR:70-85] years, n=1,800 [17.7%] female). A convolutional neural network (CNN) pre-trained using a bio-contrastive pretext on ECGs before 2015 was fine-tuned for ATTR-CM using 5-fold cross-validation and subsequently tested in an independent set of cases (139 ECGs in 47 patients; age 80 [75-86] years, n=44 (31.7% women)) and matched controls (1390 ECGs and patients) from the second half of 2023. Results: The AUROC (area under the receiver operating characteristic curve) of the AI-ECG model for discriminating ATTR-CM in the leave-out, temporally distinct dataset was 0.906 [95%CI: 0.89-0.94] (A), with a sensitivity of 0.85 [95%CI: 0.79-0.91] and specificity 0.80 [95%CI 0.78-0.82]. Conclusions: We demonstrate that AI applied directly to ECG images represents a promising and scalable approach for the screening of ATTR-CM.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Postpartum Tubal Sterilization in Sickle Cell Disease in the 2012-2019 National Inpatient Sample\nAbstract: Importance: Sickle cell disease (SCD) is associated with high-risk pregnancy and low rates of hormonal contraception use. Intersectional vulnerabilities among individuals with SCD in the United States (US) raise historically and socially contingent questions about tubal sterilization in individuals with SCD. However immediate postpartum tubal sterilization (TS) rates among individuals with SCD in the US are unknown. Objective: To compare rates of TS in deliveries to people with SCD and without SCD, and to determine the modifying effect of severe maternal morbidity (SMM) on the odds of TS. Design, Setting, and Participants: This repeated cross-sectional study used the 2012-2019 National Inpatient Sample to estimate the rate of TS among delivery hospitalizations to people with SCD, without SCD (non-SCD), Black people with and without SCD, and people with cystic fibrosis (CF). Logistic regression models estimated the odds of TS between SCD and non-SCD deliveries, SCD and non-SCD deliveries with Black race, and SCD and CF deliveries. We examined whether SMM modified the association between TS and SCD in interaction analyses. Exposure: SCD, CF Results: Among 29,822,518 deliveries, 6.7% underwent TS. Among 18,860 SCD deliveries, 8.8% underwent TS. Among 2,945 CF deliveries, 6.6% underwent TS. After adjusting for patient and hospital characteristics, SCD had higher odds of TS compared to non-SCD deliveries (aOR= 1.38 [1.06,1.79]) and in a stratified analysis of deliveries coded with Black race (aOR= 1.42 [1.06,1.90]). After adjusting for patient and hospital characteristics, there was no difference in the odds of TS between SCD or CF deliveries (aOR=1.0 [0.51,2.24]). SMM more than doubled the odds of TS in SCD deliveries (interaction: aOR=2.34 [1.57,3.47]; aOR= 2.14 [1.40,3.24] in deliveries coded with Black race). Conclusion: Even after accounting for patient and hospital characteristics, people with SCD have higher odds of immediate postpartum TS compared to control groups. SMM at delivery increased the odds of TS in SCD compared to all non-SCD deliveries and compared to CF deliveries. Possibly SMM severity, patient preference, or clinician recommendations inform this finding. SMM is 3-7 times more common in SCD than non-SCD pregnancies and may be a modifiable risk factor for TS in SCD deliveries.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Assessing sustained attention processes and related cerebral activations in typically developing adolescents using the gradual-onset continuous performance task (gradCPT)\nAbstract: Introduction: Sustained attention and inhibition processes are fundamental components of attention that mature during adolescence, a transitive period between childhood and adulthood characterized by a rapid behavioral and cognitive development. The current study aimed to provide a better understanding of sustained attention and inhibition processes in typically developing adolescents (n = 26) aged 11-18. Methods: Functional magnetic resonance images (fMRI) were acquired during two different modalities (the face and the scene) from a previously validated gradual-onset continuous (gradCPT) paradigm to evaluate sustained attention performances. In addition, we performed linear regression analyses to investigate how cerebral activation varied as a function of covariates of interest. Results: We showed a bilateral fronto-parieto-occipito brain activation during response inhibition regardless the type of task. Participants demonstrated better behavioral performances during the scene gradCPT. We observed a mainly left-lateralized pattern of activation in a fronto-cingulo-cerebellum area during the face gradCPT and an extended bilateral fronto-temporo-parieto-occipital activation during the scene gradCPT. Finally, we found associations between brain activity and behavioral attentional responses. Conclusion: This study gives a better understanding of the neural correlates of sustained attention and inhibition in a typically developing adolescent population.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Concordance between Infectious Disease Society of America Criteria for Urinary Tract Infection Testing and Treating Vs. Emergency Department Actual Practice\nAbstract: Introduction Symptomatic and asymptomatic urinary tract infections (UTIs) are common. The Infectious Disease Society of America (IDSA) discourages testing and treating ASB other than in pregnant women during routine obstetric visit screening and patients undergoing urologic procedures with expected mucosal bleeding. Unnecessary urinalysis (UA) and inappropriate antibiotic use persist in Emergency Departments (EDs). This study aims to evaluate UA testing and antibiotic treatment patterns for ASB in an urban ED, assessing adherence to IDSA guidelines and setting baseline rates for an educational intervention to align testing and treatment with IDSA guidelines. Methods We conducted a 15-month study to assess adherence to IDSA guidelines for proper UTI screenings and prescribing. We reviewed records of 50 adult patients who had a UA at the Long Island Jewish Medical Center ED to determine whether they met IDSA criteria for UA testing and appropriate antibiotic use. Patients with sepsis or other conditions requiring empiric antibiotics were excluded. We performed a univariate analysis to describe the population and factors associated with treating urinalysis findings. Statistical significance was set at p <0.05. Results Sixty-four percent of patients were asymptomatic and 36% were symptomatic. None of the asymptomatic patients met IDSA criteria for UA testing. Symptomatic patients were nearly-statistically more likely than asymptomatic patients to have a positive UA ( 72.2% vs. 43.8%, p = 0.06), and were more-often prescribed antibiotics for a positive UA (61.5% vs. 14.3%; p = 0.0128). They were also more-often prescribed antibiotics for a negative UA (20.0% vs. 0%; p = 0.05). Discussion The study findings revealed significant discordance between IDSA guidelines and current ED practices, with 64% of UA tests deemed unnecessary for ASB patients. These results align with previous studies highlighting the prevalence of over-testing with UAs and over-treatment of ASB. Unnecessary testing and inappropriate antibiotic treatment lead to increased costs and risks of antibiotic resistance, adverse drug events, and Clostridium difficile infection. This study highlights the necessity for planned educational initiatives to reduce unnecessary UAs and treatment and improve adherence to evidence-based guidelines.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Anthocyanin-rich extract mitigates the contribution of the pathobiont genus Haemophilus in mild-to-moderate ulcerative colitis patients\nAbstract: Objective: Anthocyanins (AC) have been shown to elicit anti-inflammatory and antioxidant effects in several animal models of colitis. Furthermore, AC lowered biochemical disease activity in our double-blind randomized trial in patients with ulcerative colitis. Here, we report on the changes in the faecal microbiome composition in the patient upon AC exposure in this trial. Methods: Ulcerative colitis patients received a 3g daily dose of an AC-rich bilberry extract (ACRE) for eight weeks. We determined the microbiome composition in longitudinal stool samples from 24 patients and quantified the degree of change over time. We also correlated the relative abundances of individual microbial taxa at different time points to faecal concentration measurements of calprotectin. Results: Microbiome compositions did not change over time as a result of the intervention, both in terms of alpha and beta diversity. Before the intervention, Haemophilus parainfluenzae was positively correlated with faecal calprotectin concentrations, and this correlation persisted in placebo-treated subjects throughout the study. In contrast, the correlation between H. parainfluenzae and the concentration of faecal calprotectin vanished in ACRE-treated subjects, while the relative abundance of H. parainfluenzaae did not change. Conclusion: Our results suggest that ACRE treatment mitigates the contribution of H. parainfluenzae to inflammation. Further research is warranted to better comprehend the role of microbial composition in response to medical therapy including AC-rich extract in ulcerative colitis patients.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Prevalence of dementia risk factors in a memory clinic setting: The Oxford Brain Health Clinic\nAbstract: With promising disease-modifying therapies (DMTs) emerging and good evidence to support risk reduction in the delay of dementia onset and progression, it is important to understand the profile of patients attending memory assessment services to estimate what proportion of patients might benefit from different types of interventions. The Oxford Brain Health Clinic (OBHC) is a psychiatry-led, clinical-research service that offers memory clinic patients detailed clinical assessments and equal access to research opportunities as part of their secondary care pathway. In this work, we describe the characteristics of OBHC patients in terms of demographics, diagnoses and prevalence of potentially modifiable risk factors compared with a cohort of healthy volunteers and the average memory clinic population. Our results suggest that high research consent rates (91.5%) in the OBHC resulted in a highly representative cohort of the clinical population. Based on Lecanemab trial inclusion criteria, 24.6% of the OBHC population may be suitable for further investigation into DMTs. Furthermore, 67.4% of OBHC patients have at least one potentially modifiable risk factor that may benefit from lifestyle interventions, particularly those focused on depression, sleep and physical activity.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Persistently high TB prevalence in Nairobi County neighbourhoods, 2015-2022\nAbstract: National and sub-national population-based surveys, when performed at intervals, may assess important changes in TB prevalence. In 2022 we re-surveyed nine Nairobi County neighbourhoods that were previously surveyed in 2015. We aimed to determine pulmonary TB prevalence, compare prevalence to 2015 estimates, and evaluate changes in risk groups. Participants who reported cough of any duration and/or whose chest x-ray suggested TB submitted sputum for smear microscopy, Xpert Ultra, and liquid culture. We defined prevalent TB as Mycobacterium tuberculosis detection by sputum Xpert or culture, excepting individuals who were only trace positive. Our methods differed from 2015, which used solid media, Xpert MTB/RIF, and cough duration >2 weeks. We calculated TB prevalence using random-effects logistic regression models with missing value imputations and inverse probability weighting. In 2022 among 6369 participants, 1582 submitted >1 sputum sample, among whom 42 (2{middle dot}7%) had TB, a weighted TB prevalence of 806/100,000 (95% confidence interval (CI), 518-1096). An additional 31 (2{middle dot}0%) participants tested Ultra trace-positive/culture-negative. For comparison to 2015, we excluded 2022 participants (n=2) whose only criterion for sputum was cough <2 weeks. There was no evidence for a decline in overall TB prevalence from 2015 to 2022. TB prevalence among men was high (1368/100,000) but unchanged compared to 2015 (p-value = 0{middle dot}50). The age group with the highest estimated prevalence remained people ages 45-54 years. Among people with prevalent TB who reported cough, 76% had not sought health care. Dissimilar from other serial surveys that showed declines in TB prevalence, we found persistently high TB prevalence over a 7-year period in Nairobi County. Limitations of this study include changes in methodology between the two surveys and complex effects of the COVID-19 pandemic.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Yearly trends in cause-specific age-adjusted mortality from 1999-2020 in Black and White Non-Hispanic Americans\nAbstract: From 1999 to 2020, disparities in all-cause age-adjusted mortality rates between Black and White Non-Hispanic Americans decreased then stagnated and increased. Consistent improvement in racial disparities were made for cancer, HIV, and sepsis regardless of all-cause trends. In contrast, conditions like heart disease and external causes of death mirrored the overall trends.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: End-to-End Machine Learning based Discrimination of Neoplastic and Non-neoplastic Intracerebral Hemorrhage on Computed Tomography\nAbstract: Purpose: To develop and evaluate an automated segmentation and classification tool for the discrimination of neoplastic and non-neoplastic intracerebral hemorrhage (ICH) on admission Computed Tomography (CT) utilizing images containing hemorrhage and perihematomal edema. Materials and Methods: The models were developed and evaluated using a retrospective dataset of patients who presented with acute ICH of unknown cause upon admission, using CT scans obtained from a single institution between January 2016 and May 2020 for both training and testing. Etiology of ICH were binarized into non-neoplastic and neoplastic ICH according to follow-up MRI results based on the ATOMIC ICH classification. Masks for ICH and PHE were manually segmented. Two separate models were trained: 1) An nnU-Net segmentation model 2) A ResNet-34 classification model. An end-to-end tool was evaluated by concatenating the two models which allowed the segmentation model to preprocess the images for the classification model. Performance enhancement was assessed by fine-tuning the model on a randomly selected, small subset of the external cohort. To assess the model's generalizability, the performance was additionally validated on an external dataset. Evaluation metrics were accuracy (Acc), area under the curve (AUC) and corresponding sensitivities and specificities. Results: A total of 291 patients were included of whom 116 (39.86%) presented with neoplastic and 175 (60.14%) with non- neoplastic ICH. The end-to-end classification tool achieved an Acc of 86% and an AUC of 85% with a sensitivity and specificity of 80% and 93% in the test set. On the external validation cohort (n=58), the classification pipeline achieved an AUC of 68% and Acc of 66% (sensitivity 64%; specificity 67%). Fine-tuning on a selected small subset of the external cohort enhanced performance, achieving an AUC and accuracy of 70% (sensitivity 70%; specificity 71%). Conclusion: An end-to-end classification tool achieved a high diagnostic performance and generalizability in classifying neoplastic from non-neoplastic ICH on CT, suggesting a robust framework for a potential clinical implementation as a decision-aided tool in early ICH management.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Prism Adaptation-Induced Modulation of Cortical Excitability of Upper and Lower Limb Muscles is Enhanced with Electrical Stimulation\nAbstract: Background: Prism adaptation (PA) is a behavioral training paradigm shown to induce immediate changes in visuospatial and motor function. PA may modulate the input and output stages of processing spatial cognitive information. However, the cortical plasticity mechanisms related to the behavioral effects of PA are poorly understood. Non-invasive sensorimotor electrical stimulation (Stim) may upregulate corticomotor excitability and is commonly used as a therapeutic adjunct during motor training. Objective: To evaluate the effects of combining PA with Stim on corticomotor excitability and visuo-spatial-motor behavior. Methods: We used a repeated-measures design to evaluate the effects of 1 session of PA with and without Stim in 15 young able-bodied individuals (18-35 years). Corticomotor output, intracortical inhibition, and facilitation were evaluated before and after PA with motor-evoked potentials (MEPs) elicited from single and paired-pulse transcranial magnetic stimulation (TMS) delivered to M1 hotspots of bilateral upper and left lower limbs. Results: Behaviorally, both PA+Stim and PA+Sham showed significant sensorimotor aftereffects (visuo-spatial and proprioceptive pointing). Neurophysiologically, suprathreshold MEP amplitude increased in the left first dorsal interossei, left tibialis anterior, and left soleus following the PA+Stim condition compared to the PA+Sham condition. Additionally, following PA+Stim, Short Interval Intra-Cortical Inhibition (SICI) increased in the left tibialis anterior and left soleus. Conclusion: Although both PA+Stim and PA+Sham had similar behavioral aftereffects, only PA+Stim increased cortical excitability in M1 representations of the left upper and lower limb (toward the direction of the PA aftereffect), suggesting that PA+Stim may elicit greater neurophysiological changes than PA alone. Keywords: visuomotor adaptation; sensorimotor adaptation; non-invasive brain stimulation; young adults; neuroplasticity; electrical stimulation.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Change in Social Participation of Adults with Spinal Cord Injury During the First Two Waves of the COVID-19 Pandemic in Canada\nAbstract: Introduction: The change in environmental and social context measures during the COVID-19 pandemic affected daily activities of people with spinal cord injury (SCI), their inter-actions within the community, and consequently their social participation during the first wave of the pandemic. However, there is little information about the changes in social participation as the pandemic evolved in Canada. Objective: To describe the change in social participation of adults with SCI after the first two years of the COVID-19 pandemic in Canada. Methods: A follow-up from a previous study exploring the social participation of adults with SCI living during the first wave was conducted eight months later (second wave). Social participation was measured using the Assessment of Life Habits (LIFE-H 4.0) and Measure of Quality of the Environment (MQE)) among 18 adults with SCI. Results: Participants reported increases between both waves of COVID-19 in life habits categories, including mobility, personal care and health, nutrition, and recreation. New environmental factors were identified as facilitators, including the increased availability of businesses in the community. Conclusion: These findings suggest that people with SCI increased fulfillment and satisfaction of certain life habits. Reduced environmental barriers and increased facilitators improved social participation as the pandemic evolved.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Rapid, Non-Invasive Breath Analysis for Enhancing Detection of Silicosis Using Mass Spectrometry and Interpretable Machine Learning\nAbstract: Occupational lung diseases, such as silicosis, are a significant global health concern, especially with increasing exposure to engineered stone dust. Early detection of silicosis is helpful for preventing disease progression, but existing diagnostic methods, including X-rays, CT scans, and spirometry, often detect the disease only at late stages. This study investigates a rapid, non-invasive diagnostic approach using atmospheric pressure chemical ionization-mass spectrometry (APCI-MS) to analyse volatile organic compounds (VOCs) in exhaled breath from 31 silicosis patients and 60 healthy controls. Six different interpretable machine learning (ML) models with Shapley additive explanations (SHAP) were applied to classify these samples and determine VOC features that contribute the most significantly to model accuracy. The extreme gradient boosting (XGB) classifier demonstrated the best performance, achieving an area under the receiver-operator characteristic curve of 0.933 with the top ten SHAP features. The m/z 442 feature, potentially corresponding to leukotriene-E3, emerged as a significant predictor for silicosis. The VOC sampling and measurement process takes less than five minutes per sample, highlighting its potential suitability for large-scale population screening. Moreover, the ML models are interpretable through SHAP, providing insights into the features contributing to the model's predictions. This study suggests that APCI-MS breath analysis could enable early and non-invasive diagnosis of silicosis, helping to improve disease outcomes.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Head kinematics associated with off field head injury assessment (HIA1) events in a season of English elite-level club men's and women's rugby union matches.\nAbstract: Objectives: To investigate head kinematic variables in elite men's and women's rugby union and their ability to predict player removal for an off-field (HIA1) head injury assessment. Methods: Instrumented Mouthguard (iMG) data were collected for 250 men and 132 women from 1,865 and 807 player-matches, respectively, and synchronised to video-coded match footage. Head peak resultant linear acceleration (PLA), peak resultant angular acceleration (PAA) and peak change in angular velocity (dPAV) were extracted from each head acceleration event (HAE). HAEs were linked to documented HIA1 events, with ten logistical regression models for men and women, using a random subset of non-case HAEs, calculated to identify kinematic variables associated with HIA1 events. Receiver operating characteristic curves (ROC) were used to describe thresholds for HIA1 removal. Results: Increases in PLA, and dPAV were significantly associated with an increasing likelihood of HIA1 removal in the men's game, with an OR ranging from 1.05-1.12 and 1.13-1.18, respectively. The optimal values to maximise for both sensitivity and specificity for detecting an HIA1 were 1.96krad.s-2, 24.29g, and 14.75rad.s-1 for PAA, PLA, and dPAV respectively. Only one model had any significant variable associated with increasing the likelihood of a HIA1 removal in the women's game - PAA with an OR of 8.51 (1.23-58.66). The optimal values for sensitivity and specificity for women were 2.01krad.s-2, 25.98g, and 15.38rad.s-1 for PAA, PLA, and dPAV respectively. Conclusion: PLA and dPAV were predictive of men's HIA1 events. Further HIA1 data are needed to understand the role of head kinematic variables in the women's game. The calculated spectrum of sensitivity and specificity of iMG alerts for HIA1 removals in men and women present a starting point for further discussion about using iMGs as an additional trigger in the existing HIA process.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Brain-heart interactions are associated with mortality and acute encephalopathy in ICU patients with severe COVID-19\nAbstract: Objective: Research in critical care has revealed the significance of autonomic dysfunctions, and more recently of brain-heart interactions, as valuable biomarkers for evaluating patients' physiological status. These biomarkers provide insights into consciousness levels, severity, and outcomes. This study aims to determine the potential of these biomarkers in predicting the mortality and neurological outcome of severe COVID-19 patients. Methods: We examined severe COVID-19 patients who required mechanical ventilation and observed them both during sedation and after sedation cessation. Standard EEG and ECG recordings were conducted at bedside, from which 5 minutes of continuous data were analyzed. Using a synthetic data generation model, we evaluated bidirectional brain-heart interactions from EEG power and heartbeat dynamics series. Results: Our findings indicate that brain-heart interactions, especially involving cardiac parasympathetic activity, can provide information about patients' severity. We observed correlations with acute encephalopathy duration (coma and delirium), particularly evident in top-down markers (from brain to heart) while bottom-up signaling (from heart to brain) correlated with ICU mortality. Additionally, we noted stronger modulation of brain-heart interactions in milder patients when comparing sedation versus non-sedation conditions, compared to those in more severe states. Conclusions: Our results imply that autonomic dysfunctions, as measured through brain-heart interactions, can track the pathophysiology of comatose states following COVID-19 infection. Significance: These findings highlight the potential of brain-heart interactions as an integrated marker for autonomic function in critical care, offering a more accurate assessment of patient severity and outcomes compared to isolated cardiac or brain measures.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Social Determinants of Health and Functional Brain Connectivity Predict Long-Term Physical Activity Adherence in Older Adults After a New Cardiovascular Diagnosis\nAbstract: Physical activity is a crucial modifiable risk factor for preventing cognitive decline, stroke, and dementia. A new cardiovascular disease diagnosis offers an opportunity to improve health outcomes through lifestyle changes. However, sustaining behavior change, even with effective exercise interventions, remains challenging. Understanding the neurobehavioral mechanisms behind successful long-term behavior change can help tailor therapeutic strategies to enhance physical activity and reduce dementia and stroke risks in older adults. This study followed 295 cognitively unimpaired older adults from the UK-Biobank cohort, all newly diagnosed with cardiovascular disease, over five years. We aimed to predict future changes in moderate-to-vigorous physical activity (MVPA) engagement using baseline resting-state functional connectivity, along with sociodemographic factors (e.g., age, sex, socioeconomic status), behavioral characteristics (e.g., general health, pain, depression), cognitive function (e.g., attention, fluid intelligence), social factors (e.g., networks and support), and environmental context (e.g., access to green spaces). Our findings revealed that greenspace percentage, social support, and retirement status significantly predicted increased MVPA at follow-up. A support vector machine (SVM) model, utilizing baseline functional connectivity matrices, achieved 84% accuracy in predicting successful MVPA engagement at follow-up. Key features contributing to the model's accuracy included functional connectivity between brain regions located in distinct large-scale networks, particularly the heteromodal cortex. Notably, the default mode network, frontoparietal control network, and salience/ventral attention network were prominently involved. These results underscore the importance of social and structural determinants of health, such as greenspace and social support, in promoting physical activity behavior change. Furthermore, they provide preliminary evidence of neural mechanisms that may underlie successful lifestyle modifications, informing future interventions to prevent vascular contributions to late-life cognitive decline.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Information processing speed modulation by electrical brain stimulation in multiple sclerosis: Towards individually-tailored protocols\nAbstract: Information processing speed (IPS) is a core cognitive process, highly relevant in everyday-life and the most frequent and disabling cognitive symptom in patients with Relapsing Multiple Sclerosis (pwRMS). By using regionally-specific focalized transcranial direct current stimulation (tDCS) in healthy individuals and pwRMS, we provide causal evidence for superior parietal lobe (SPL) involvement in IPS and identified a clinical predictor of tDCS-response in pwRMS. The study employed a registered, randomized, sham-tDCS controlled, three-way blinded, cross-over trial and a mixed-factors design with eight arms [between-subjects: group (pwRMS, controls); tDCS-polarity (excitatory, inhibitory); within-subjects: stimulation (active-, sham-tDCS)]. Concurrently with tDCS, participants completed a computerized version of the Symbol-Digit-Modalities-Test (SDMT), the current gold standard for quantifying IPS impairment in pwRMS. Bayesian modeling with generalized linear mixed models provided strong evidence for polarity specific modulation of IPS by SPL-tDCS and a double-dissociation of stimulation response in pwRMS and controls. Healthy individuals showed the canonical pattern of significantly enhanced and reduced processing speed during excitatory or inhibitory tDCS. In pwRMS, a re-versed pattern was found and tDCS-response was predicted by baseline SDMT performance; i.e., more or less impaired patients benefited from inhibitory or excitatory tDCS, respectively. Our results provide direct causal evidence for SPL involvement in IPS in health and disease and suggest that the degree of IPS impairment in pwRMS reflects compensatory or dysfunctional neuroplastic processes that can be counteracted in a polarity specific way. Identified standardized transition scores for the effectiveness of excitatory or inhibitory tDCS will inform future individually-tailored stimulation protocols in pwRMS.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: TMS target comparison identifies motor network reorganization associated with behavioral improvement in writer's cramp dystonia: A randomized, double-blind, sham-controlled clinical trial\nAbstract: Background: Writer's cramp (WC) dystonia is an involuntary movement disorder with distributed abnormalities in the brain's motor network. Prior studies established the potential for repetitive transcranial magnetic stimulation (rTMS) to either premotor cortex (PMC) or primary somatosensory cortex (PSC) to modify symptoms. However, clinical effects have been modest with limited understanding of the neural mechanisms hindering therapeutic advancement of this promising approach. Objective: This study aimed to understand the motor network effects of rTMS in WC that correspond with behavioral efficacy. We hypothesized that behavioral efficacy is associated with modulation of cortical and subcortical regions of the motor network. Methods: In a double-blind, cross-over design, twelve WC participants underwent weekly 10 Hz rTMS in one of three conditions (Sham-TMS, PSC-TMS, PMC-TMS) while engaged in a writing task to activate dystonic movements and measure writing fluency. Brain connectivity was evaluated using task-based fMRI after each TMS session. Results: 10 Hz rTMS to PSC, but not PMC, significantly improved writing dysfluency. PSC-TMS also significantly weakened cortico-basal ganglia, cortico-cerebellum, and intra-cerebellum functional connectivity (FC), and strengthened striatal connectivity relative to Sham. Increased PSC BOLD activity was associated with reduced dysfluent writing behavior. Conclusions: 10 Hz rTMS to PSC improved writing dysfluency by redistributing motor network connectivity and strengthening somatosensory-parietal connectivity. A key signature for effective stimulation at PSC and improvement in writing dysfluency may be strengthening of intra-cortical connectivity between primary somatosensory and superior parietal cortices. These findings offer mechanistic hypotheses to advance the therapeutic application of TMS for dystonia.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Determinants of acute malnutrition among children under five in arid regions, Kenya\nAbstract: Acute malnutrition, defined as a weight-for-height Z-score below -2 standard deviations of the WHO growth standards, is prevalent among children in low- and middle-income countries. Our study aimed to identify immediate, underlying, and basic determinants of acute malnutrition among children in Turkana and Samburu, two arid and semi-arid regions in Kenya. Data are from a longitudinal study that recruited children under 3 years of age, with follow-up every 4 months over six waves. Generalized estimating equations were used to assess risk factors of acute malnutrition in this population. Among immediate factors, children who recently experienced diarrhea had 19% and 23% higher odds of acute malnutrition and those who consumed animal-source foods had 12% and 22% lower odds of acute malnutrition in Turkana and Samburu, respectively. Among underlying factors, children in Turkana whose caregivers used alcohol had 32% higher odds of acute malnutrition. Among basic factors, children in Turkana whose caregivers had 3-5 or 6 or more children had 39% and 70% higher odds, whereas those in female-headed households had 34% and 81% higher odds of acute malnutrition in Turkana and Samburu, respectively. Children in Turkana fisherfolk communities had 36% higher odds of acute malnutrition compared with those in urban or peri-urban areas. Key risk factors for acute malnutrition included child diarrhea, caregivers use of alcohol (in Turkana), caregivers number of children, female-headed households, and fisherfolk livelihood (in Turkana), while consuming animal-source foods was associated with lower risk. Interventions should target these intersecting factors to reduce acute malnutrition in these counties.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Spatial and temporal changes in choroid morphology associated with long-duration spaceflight\nAbstract: Purpose The development of neuro-ophthalmic symptoms during long-duration spaceflight presents major risks to astronauts and their missions. Amid efforts to understand spaceflight associated neuro-ocular syndrome (SANS), uncovering the role of the choroid in its etiology is challenged by the accuracy of image segmentation. The present study extends deep learning-based choroid quantification from optical coherence tomography (OCT) to the characterization of pulsatile and topological changes in the macular plane and investigates changes in response to prolonged microgravity exposure. Methods We analyzed OCT macular videos and volumes acquired from astronauts before, during and after long-duration spaceflight. Deep learning models were fine-tuned for choroid segmentation and combined with further image processing towards vascularity quantification. Statistical analysis was performed to determine changes in time-dependent and spatially averaged variables from preflight baseline for OCT data. Results For 13 astronauts with a mean age of 47 {+/-} 9 years, there were significant increases in choroid thickness (CT) and luminal area (LA) of 14 {+/-} 25 m (P = 0.01) and 0.11 {+/-} 0.2 mm2 (P = 0.009) and no significant change in choroid vascularity index (CVI) averaged over OCT macular video segments. There was no significant change in the amplitude of pulsatile CT fluctuation ({Delta}CT), but significant increases in both pulsatile LA ({Delta}LA) and CVI ({Delta}CVI) of 0.077 {+/-} 0.14 mm2 (P = 0.009) and 0.03 {+/-} 0.06 (P = 0.01), respectively. For a subgroup of 6 astronauts for which inflight imaging was available, choroid volume, luminal volume and CVI over the macular region all increased significantly during spaceflight with the greatest differences reached at 90 days following launch. Conclusions The findings show that localized choroid pulsatile changes occur following prolonged microgravity exposure. They suggest that the choroid vessels expand in a manner similar to the choroid layer across the macular region during spaceflight, accompanied by a relative increase in the space they occupy. The methods developed provide new tools and avenues for studying and establishing effective countermeasures to risks associated with long-duration spaceflight.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: SeqFirst: Building equity access to a precise genetic diagnosis in critically ill newborns\nAbstract: Access to a precise genetic diagnosis (PrGD) in critically ill newborns is limited and inequitable because complex inclusion criteria used to prioritize testing eligibility omits many patients at high risk for a genetic condition. SeqFirst-neo is a program to test whether a genotype-driven workflow using simple, broad exclusion criteria to assess eligibility for rapid whole genome sequencing (rWGS) increases access to a PrGD in critically ill newborns. All 408 newborns admitted to a neonatal intensive care unit between January 2021 and February 2022 were assessed and of 240 eligible infants, 126 were offered rWGS (i.e., intervention group [IG]) and compared to 114 infants who received conventional care in parallel (i.e., conventional care group [CCG]). A PrGD was made in 62/126 (49.2%) IG neonates compared to 11/114 (9.7%) CCG infants. The odds of receiving a PrGD was ~9 times greater in the IG vs. the CCG, and this difference was maintained at 12 months follow up. Access to a PrGD in the IG versus CCG differed significantly between infants identified as non-white (34/74, 45.9% vs. 6/29, 20.7%; p=0.024) and Black (8/10, 80.0% vs. 0/4; p=0.015). Neonatologists were significantly less successful at predicting a PrGD in non-white than non-Hispanic white patients. Use of a standard workflow in the IG with a PrGD revealed that a PrGD would have been missed in 26/62 (42%) of infants. Use of simple, broad exclusion criteria that increases access to genetic testing significantly increases access to a PrGD, improves access equity and results in fewer missed diagnoses.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Risk of Incident Cytopenia in Clonal Hematopoiesis\nAbstract: Clonal hematopoiesis of indeterminate potential (CHIP) is an asymptomatic condition associated with elevated risk for myeloid neoplasms (MN). Patients with CHIP and cytopenia are at greater risk of MN. Quantifying the incidence of cytopenia and identifying risk factors among CHIP patients is critical for improving clinical management. We analyzed sequencing data from 805,249 participants in the NIH All of Us Research Program (AoU), Vanderbilt's BioVU repository, and UK Biobank (UKB). Genetic mutations, laboratory values, and MN diagnoses were included in survival analyses to determine predictors of cytopenia in individuals with CHIP and matched controls. The cohort contained 9,374 CHIP cases and 24,749 controls. Cytopenia occurred in 13.5% of cases and 11.6% of controls (HR = 1.17, 95% confidence interval: 1.10 -- 1.25, P=2.5 x 10-6). Cytopenia risk factors included smoking, male sex, variant allele frequency [&ge;] 0.20, age [&ge;] 65, mean corpuscular volume [&ge;] 100 femtoliters, red cell distribution width [&ge;] 15%, mutations in high-risk CHIP genes, and [&ge;] 2 CHIP mutations. In BioVU, 45% of participants with [&ge;] 4 risk factors progressed to cytopenia within two years. Individuals with CHIP and cytopenia progressed to MN at a rate of ~2% per year, compared to <0.1% per year for those without cytopenia. Longitudinal analysis across three cohorts demonstrated an increased risk of cytopenia in CHIP patients and identified those at highest risk. These findings suggest that cytopenia is a critical step in progression from CHIP to MN, underscoring its utility as an endpoint in cancer prevention trials for CHIP patients.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Genetic Association of Glutathione S-Transferase Omega 2 (GSTO2) Gene Variant (rs156697) with Chronic Kidney Disease Patients of Pakistani Origin\nAbstract: Chronic Kidney Disease (CKD) poses a significant global health challenge. The kidneys can be compromised by internal diseases, environmental influences and various genetic factors. Among the genetic factors, particularly, GSTO2 gene plays a crucial role in cellular processes and enzyme performance, which can lead to interstitial kidney damage and contribute to CKD. The current study investigates the genetic association between the GSTO2 gene variation (rs156697) and CKD in the Punjabi community. This study examined the association between the GSTO2 gene locus (c.424A>T) and CKD in a Punjabi population using ARMS-PCR genotyping. A total of 50 samples were analyzed, revealing 19 homozygous wild-type, 25 heterozygous and 6 homozygous mutants. PLINK data analysis toolset indicated an insignificant genotypic association with CKD phenotype with an observed p-value of 0.7177 and an odds ratio of 0.85. Similarly, the alternative allele frequency of 0.34 and 0.375 was also observed in CKD and normal controls respectively. In the future, further studies with more genetic markers along with functional studies with large sample data may be performed for robust statistical evidence of the association of subject gene with CKD patients in Pakistan.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Biomedical Text Normalization through Generative Modeling\nAbstract: ABSTRACT Objective: Around 80% of electronic health record (EHR) data consists of unstructured medical language text. By its nature, this text is flexible and inconsistent, making it challenging to use for clinical trial matching, decision support, and predictive modeling. In this study, we develop and assess text normalization pipelines built using large-language models. Materials and Methods: Here, we evaluated four LLM-based normalization strategies: Zero-Shot Recall, Prompt Recall, Semantic Search, and Retrieval-Augmented Generation (RAG) and one baseline, TF-IDF based String Matching. We compared normalization performance across two datasets of condition terms mapped to SNOMED, one tailored to oncology, and one covering a wide range of medical conditions. Additionally, we benchmarked our models against the TAC 2017 drug label annotations, which normalizes terms to the Medical Dictionary for Regulatory Activities (MedDRA) Preferred Terms. Results: RAG, which effectively combines Prompt Recall and Semantic Search, was the most effective, accurately identifying the correct term 88.31% of the time for the domain-specific dataset and 79.97% for the broader dataset. Our model achieved a micro F1 score of 88.01 on task 4 of the TAC2017 conference, surpassing all other models without relying on the provided training data. Discussion: These findings demonstrate the potential of LLMs in medical text normalization. We find that retrieval-focused approaches overcome traditional LLM limitations for this task. Conclusion: Large language models combined with retrieval-augmented generation should be explored for text normalization of biomedical free text.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Derivation and validation of a prediction rule for sedative-associated delirium during acute respiratory failure requiring mechanical ventilation\nAbstract: Abstract Background: Delirium during acute respiratory failure is common and morbid. Pharmacologic sedation is a major risk factor for delirium, but some sedation is often necessary for the provision of safe care of mechanically ventilated patients. A simple, transparent model that predicts sedative-associated delirium in mechanically ventilated ICU patients could be used to guide decisions about personalized sedation. Research Question: Can the risk of sedative-associated delirium be estimated in mechanically-ventilated ICU patients? Study Design and Methods: Using the subset of patients in a previously-published ICU cohort who received mechanical ventilation, we performed backward stepwise logistic regression to derive a model predictive of sedative-associated delirium. We validated this model internally using hundredfold bootstrapping. We then validated this model externally in a separate prospective cohort of mechanically ventilated ICU patients. Results: 836 patients comprised the derivation cohort. Backwards stepwise regression produced a model with age, BMI, sepsis, SOFA, malignancy, COPD, stroke, sex, and doses of sedatives (opioids, propofol, and/or benzodiazepines) as predictors of sedative-associated delirium. The model had very good discriminative power, with an area under the receiver-operator curve (AUROC) of 0.83. Internal validation via bootstrapping showed preserved discriminatory function with an AUROC of 0.81 and graphical evidence of good calibration. External validation in a separate set of 340 patients showed good discrimination, with AUROC of 0.70. Interpretation: Sedative-associated delirium during acute respiratory failure requiring mechanical ventilation can be predicted using a simple, transparent model, which can now be validated in a prospective study.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Solid-state esophageal pressure sensor for the estimation of pleural pressure: a bench and first-in-human validation study\nAbstract: Background: Advanced respiratory monitoring through the measurement of esophageal pressure (Pes) as a surrogate of pleural pressure helps guiding mechanical ventilation in ICU patients. Pes measurement with an esophageal balloon catheter, the current clinical reference standard, needs complex calibrations and a multitude of factors influence its reliability. Solid-state pressure sensors might be able to overcome these limitations. Objectives: To evaluate the accuracy of a new solid-state Pes transducer (Pessolid). We hypothesized that measurements are non-inferior to those obtained with a properly calibrated balloon catheter (Pesbal). Methods: Absolute and relative solid-state sensor Pes measurements were compared to a reference pressure in a 5-day bench setup, and to simultaneously placed balloon catheters in 15 spontaneously breathing healthy volunteers and in 16 mechanically ventilated ICU patients. Bland-Altman analysis was performed with nonparametric bootstrapping to estimate bias and upper and lower limits of agreement (LoA). Results: Bench study: Solid-state pressure transducers had a positive bias (Psolid - Pref) of around 1 cmH2O for the absolute minimal and maximum pressures, and no bias for pressure swings. Healthy volunteers: the solid-state transducer revealed a bias (Pessolid - Pesbal) [upper LoA; lower LoA] of 1.58 [8.19; -5.03], -2.37 [3.96; -8.69] and 3.94 [11.09; -3.20] cmH2O for end-expiratory, end-inspiratory and delta Pes values, respectively. ICU patients: the solid-state transducer showed a bias (Pessolid-Pesbal) [upper LoA; lower LoA] during controlled / assisted ventilation of: -0.15 [1.39; -1.70] / -0.20 [5.02; -5.41], 0.32 [3.35; -2.72] / -0.54 [4.60; -5.68] and 0.47 [3.79; -2.85] / 0.35 [3.88; -3.18] cmH2O for end-expiratory, end-inspiratory and delta Pes values, respectively. LoA were <2cmH2O for static measurements on controlled ventilation. Conclusions: the novel solid-state pressure transducer showed good accuracy on the bench, in healthy volunteers and in ventilated ICU-patients. This could contribute to the implementation of Pes as advanced respiratory monitoring technique. Trial registration: Clinicaltrials.gov identifier: NCT05817968 (patient study). Registered on 18 April 2023.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Physical Activity, Sleep, and Quality of Life in Pulmonary Arterial Hypertension: Novel Insights from Wearable Devices\nAbstract: Background: Reduced functional capacity and poor sleep quality are common in pulmonary arterial hypertension (PAH). We aimed to determine whether Fitbit-derived activity and sleep trends provide clinically meaningful information in patients with PAH. Methods: Our prospective observational study recruited patients with PAH from across the United States. Participants with PAH wore a Fitbit device for 12-weeks at baseline and a subgroup with one-year follow-up. A matched control cohort was generated from the All of Us Research Program and we evaluated changes in patients with PAH compared to matched controls. Results: Among 109 patients with baseline monitoring, average daily steps correlated with 6MWD (r=0.66, p < 0.001) and percent rapid eye movement (REM) sleep (r=0.26, p=0.016). In 44 PAH participants who completed baseline and one-year monitoring, there was a group-time interaction for percent light sleep (p=0.024) and percent REM sleep (p=0.034), which demonstrated that sleep quality worsened in patients with PAH over one year compared to matched controls. Average daily steps in patients with PAH decreased from 5200 [IQR 3212 - 7458] at baseline to 4651 [IQR 2912 - 6827] at one year (p=0.008). Compared to matched controls, activity levels were significantly lower in patients with PAH at both time points. Conclusions: Our study demonstrated the potential clinical value of wearable devices by showing that activity and sleep quality are reduced in PAH compared to matched controls and these measures decline over time. Future studies should investigate if monitoring these health behaviors detects early functional decline and whether targeted interventions may improve outcomes.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Multiomics identifies unique modulators of calf muscle pathophysiology in peripheral artery disease and chronic kidney disease\nAbstract: BACKGROUND: Chronic kidney disease (CKD) has emerged as a significant risk factor that accelerates atherosclerosis, decreases muscle function, and increases the risk of amputation or death in patients with peripheral artery disease (PAD). However, the modulators underlying this exacerbated pathobiology are ill-defined. Recent work has demonstrated that uremic toxins are associated with limb amputation in PAD and have pathological effects in both the limb muscle and vasculature. Herein, we utilize multiomics to identify novel modulators of disease pathobiology in patients with PAD and CKD. METHODS: A cross-sectional study enrolled four groups of participants: controls without PAD or CKD (n=28), patients with PAD only (n=46), patients with CKD only (n=31), and patients with both PAD and CKD (n=18). Both targeted (uremic toxins) and non-targeted metabolomics in plasma were performed using mass spectrometry. Calf muscle biopsies were used to measure histopathology, perform bulk and single-nucleus RNA sequencing (snRNAseq), and assess mitochondrial function. Differential gene and metabolite analyses, as well as pathway and gene set enrichment analyses were performed. RESULTS: Patients with both PAD and CKD exhibited significantly lower calf muscle strength and smaller muscle fiber areas compared to controls and those with only PAD. Compared to controls, mitochondrial function was impaired in CKD patients, with or without PAD, but not in PAD patients without CKD. Plasma metabolomics revealed substantial alterations in the metabolome of patients with CKD, with significant correlations observed between uremic toxins (e.g., kynurenine, indoxyl sulfate) and both muscle strength and mitochondrial function. RNA sequencing analyses identified downregulation of mitochondrial genes and pathways associated with protein translation in patients with both PAD and CKD. Single nucleus RNA sequencing further highlighted a mitochondrial deficiency in muscle fibers along with unique remodeling of fibro-adipogenic progenitor cells in patients with both PAD and CKD, with an increase in adipogenic cell populations. CONCLUSIONS: CKD significantly exacerbates ischemic muscle pathology in PAD, as evidenced by diminished muscle strength, reduced mitochondrial function, and altered transcriptome profiles. The correlation between uremic toxins and muscle dysfunction suggests that targeting these metabolites may offer therapeutic potential for improving muscle health in PAD patients with CKD.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: A novel patient-level computational model of atrial fibrillation patterns and clinical outcomes for evaluating screening strategies\nAbstract: Background The dynamic, heterogenous nature of atrial fibrillation (AF) episodes and poor symptom-rhythm correlation make early AF detection challenging. The optimal screening strategy for early AF detection and its role in stroke prevention in different subpopulations are unknown. We developed a computational patient-level AF model able to simulate the dynamic occurrence of AF and AF-associated outcomes during the entire lifetime of a virtual patient cohort to elucidate the impact of AF screening in virtual randomized clinical trials (V-RCTs). Methods The Markov-like computer model has 7 clinical states (sinus rhythm, symptomatic/asymptomatic AF, each with/without previous stroke, and death). AF-related atrial remodeling was incorporated, which influenced the age-/sex-dependent transition probabilities between states. Model calibration/validation was performed by replicating a wide range of clinical studies. AF screening strategies and stroke rates in the presence of defined interventions were assessed in V-RCTs. Results The model simulates the entire lifetime of virtual patients with minute-level resolution and provides perfect information on the occurrence of AF episodes and clinical outcomes (stroke, death). It replicates numerous age/sex-specific episode- and population-level AF metrics, including AF incidence/prevalence, progression rates, burden, episode duration, and stroke/mortality incidence. The benefits of intermittent AF screening in V-RCTs were frequency- and duration- dependent, with systematic thrice-daily single-ECG recordings providing the highest detection rates (64.6% of AF patients being diagnosed before their symptom-based clinical diagnosis). Screening groups had comparable 5-year stroke rates and lower 25-year stroke rates than the control group. These differences were increased by more effective anticoagulation therapy, in patients with higher baseline stroke risk, or in patients with delayed clinical AF diagnosis. Conclusions We present a novel computational patient-level AF model consistent with a large body of real-world data, enabling for the first time the systematic assessment of AF-management strategies. Screening protocols with more frequent and longer monitoring have higher AF-detection rates, but stroke reduction in screening-detected individuals is highly dependent on patients' and healthcare-systems' characteristics. V-RCTs suggest that frequent AF screening with effective anticoagulation can reduce stroke incidence in patients with high likelihood of delayed AF diagnosis.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: A Prototype Machine Learning Pipeline for Assessing and Tracking Keloid Scars\nAbstract: Dysregulated wound healing, marked by excessive collagen deposition, is the hallmark to keloid scar formation. Current methods for assessing keloids in clinical settings rely heavily on subjective measures, which are prone to interrater variability. This study introduces a machine learning (ML) pipeline prototype, designed to automate the detection, measurement, and colour analysis of keloid scars. Using a convolutional neural network (CNN), the pipeline segments keloid lesions from 2D images, applies fiducial markers for accurate size measurement, and utilizes K-Means clustering for colorimetry analysis. The CNN achieved a classification accuracy of 98% on a small test dataset. Segmentation was further refined using binary masks and contour-based detection. Colorimetry analysis revealed heterogeneity in pigmentation across keloid lesions, was varied by patient skin type, and tracked changes over time. The pipeline was validated on patients over a 5 and 6 month period, accurately detecting changes in keloid size and colour. While the algorithm was highly effective in most cases, challenges were noted in patients with nascent keloid or those with dark skin tones where the contrast between keloid and skin was insufficient for accurate segmentation. Additionally, early-stage keloid detection showed inconsistencies in defining lesion boundaries, particularly when keloids expanded rapidly. Despite these limitations, the ML pipeline presents a promising tool for objective keloid assessment, offering a practical, accessible, and accurate alternative to current clinical practices.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Subphenotypes of youth-onset type 2 diabetes mellitus and their association with distal symmetrical polyneuropathy\nAbstract: Context: Individuals with youth-onset type 2 diabetes mellitus (T2DM) display substantial, but unexplained, heterogeneity in their clinical presentations and risk of complications such as diabetic neuropathy. Data-driven clustering may be useful in characterizing this heterogeneity. Objective: To identify data-driven subphenotypes of newly diagnosed youth-onset T2DM and study their association with distal symmetric polyneuropathy (DSPN) at time of diagnosis. Design: Cross-sectional Setting: USA Participants: 641 individuals with newly diagnosed T2DM aged 10-19 years from the SEARCH for Diabetes in Youth Study and the Treatment Options for Type 2 Diabetes in Adolescents and Youth (TODAY) study. Exposure(s): Body mass index, HbA1c, fasting C-peptide, systolic blood pressure, diastolic blood pressure, LDL cholesterol and HDL cholesterol Main Outcome Measures: Data-driven subphenotypes were identified from k-means clustering. The cross-sectional association of subphenotypes with DSPN, based on expert examination scores ([&ge;]2.5) from the Michigan Neuropathy Screening Instrument, were assessed using Poisson regressions with robust standard errors. Results: Among 641 youth-onset T2DM, 58.2% were female, with 38.2% of participants [&le;]13 years having average BMI of 34.5 kg/m2 (SD: 6.5 kg/m2), and average HbA1c of 6.1% (IQR: 5.6-7.0). Three youth-onset subphenotypes were identified: mild obesity related diabetes (yMOD, 48.5%), severe insulin deficient diabetes (ySIDD, 18.7%) and severe insulin resistant diabetes (ySIRD, 32.7%). After adjusting for covariates, the prevalence of abnormal DSPN were 2.58 (95%CI: 1.74, 3.81) and 2.02 (95%CI: 1.40, 2.93) times among those classified as the ySIDD and ySIRD subphenotypes, relative to the yMOD subphenotype. Conclusions: Youth-onset T2DM consisted of heterogeneous clinical subphenotypes with differences prevalence of DSPN. Management of youth-onset T2DM may need to consider strategies tailored to each subphenotype. Keywords: precision medicine, diabetes endotypes, clusters\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Outcomes of endoscopic ultrasound-guided transmural drainage for postoperative peripancreatic fluid collection with an external drainage-first approach\nAbstract: Abstract Objectives: Endoscopic ultrasound-guided transmural drainage (EUS-TD) is widely performed to treat postoperative peripancreatic fluid collection (POPFC). Recent reports on EUS-TD lack a consensus on stent selection. This study aimed to assess the efficacy of EUS-TD for POPFC using an external drainage-first approach. Methods: We retrospectively examined the medical records of patients with POPFC treated with EUS-TD using external drainage between October 2016 and July 2024. Technical success was defined as successful placement of the external drainage. Clinical success was defined as the disappearance of fluid collection on follow-up computed tomography without additional intervention. Results: This study included 14 patients. The median duration from surgery to endoscopic treatment was 13 (range: 11-26) days. The median procedural time was 26 (range: 13-35) min. The technical success rate was 100%, and 6 Fr endoscopic nasocystic drainage was performed in all patients. The clinical success rate was 64.3%, and no adverse events were observed. In 35.7% (5/14) of the patients, additional endoscopic internal drainage was required; the POPFC was successfully controlled in all patients. The reasons for conversion to internal drainage were prolonged inflammation in one patient, remaining fluid collection in one patient, and requests from surgeons in three patients. Conclusions: EUS-TD for POPFC with external drainage proved to be safe and effective, with a short procedure time. However, in certain patients, additional internal drainage is required.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Hydroxyzine in Acute Hypertension: A Cohort Study.\nAbstract: Captopril, an ACE inhibitor, is widely used for acute hypertensive episodes, while hydroxyzine, an antihistamine with sedative and anxiolytic effects, is also employed in Poland despite limited research on its efficacy for this purpose. o our knowledge, no prior studies have specifically investigated the direct impact of hydroxyzine on hypertension, despite its common use in Polish medical practice. A retrospective cohort analysis of 2144 patients who required emergency intervention for hypertension or had high blood pressure during other interventions. Effectiveness was based on blood pressure improvement post-intervention. 84.28% of interventions were performed by basic emergency teams, and 15.72% by specialist teams. Captopril and hydroxyzine were the most commonly used medications. Blood pressure improved in 36.61% of patients, with no improvement in 41.47%. Hydroxyzine was more frequently administered by paramedic teams, while captopril was favored by specialist teams. There were no statistically significant differences in captopril use across age groups, with approximately 80% receiving it. Hydroxyzine use did not vary significantly by age, though its overall use was low. Hydroxyzine showed a non-significant trend towards greater blood pressure reduction compared to captopril. Captopril is the primary treatment for acute hypertension in emergency settings, but hydroxyzine may have potential benefits, especially in patients with anxiety-related hypertension. However, due to the lack of significant evidence and current guidelines advising against hydroxyzine in elderly patients, further research is needed to establish protocols and optimize hypertension treatment strategies. Additionally, incorporating anxiolytics into guidelines and an official statement on their use would aid clinical practice.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Outcomes of Endovascular Thrombectomy in Patients with Cerebral Venous Thrombosis: A Cohort Study of 36,005 Patients\nAbstract: Introduction/Purpose: Cerebral venous thrombosis (CVT) is traditionally managed with anticoagulation. However, for patients who fail anticoagulation or with severe symptoms, endovascular thrombectomy (EVT) has emerged as a treatment option. Objective: This study evaluates outcomes, complications, and comorbidities associated with EVT in patients with CVT. Methods: The 2015-2019 National Inpatient Sample (NIS) was queried for patients admitted to hospitals with ICD-10 diagnosis codes for CVT and procedure codes for EVT. Demographics, comorbidities, complications, and discharge dispositions were compared between patients undergoing EVT and medical management. Multivariate logistic regression determined independent predictors of outcomes, controlling for age, gender, cerebral edema/herniation, hemorrhage, mechanical ventilation, Elixhauser Comorbidity Index, and National Institute of Health Stroke Scale (NIHSS) severity. Odds ratios (OR) were calculated for each complication/outcome. All statistical analyses were performed using Statistical Product and Service Solutions (SPSS) Statistical Software Version 29. Results: A total of 36,005 patients with CVT were identified. Of those, 325 (0.9%) underwent EVT. Patients who underwent EVT were older (49.28{+/-}19.08 years vs 43.62{+/-}23.29 years, p < 0.001), more likely to be female (61.5% vs. 54.8%, p = 0.016), and had higher rates of diabetes mellitus (18.5% vs. 13.5%, p = 0.012), hypertension (47.7% vs. 31.3%, p < 0.001), and obesity (13.8% vs. 8.1%, p < 0.001). They also had higher NIHSS scores (13.15 vs. 6.17, p < 0.001). EVT was associated with higher in-hospital mortality (15.4% vs. 4.6%, OR 1.627, p = 0.007) and lower routine discharge (29.2% vs. 57%, OR 0.599, p < 0.001). Conclusions: EVT patients had more severe conditions and higher mortality, potentially reflecting baseline severity rather than risks of EVT itself. Careful patient selection is crucial to improving outcomes in this population. Further studies are needed to refine patient selection criteria for EVT in CVT.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Depth diagnosis of colorectal cancer located on a colorectal fold by quantitative evaluation of a width of colorectal - fold lateral contour using a lateral split-view computed tomographic air-contrast enema image\nAbstract: Purpose: To investigate the usefulness of quantitative evaluation of a width of lateral contour on a lateral split-view computed tomographic air-contrast enema (CT enema) image for diagnosing the invasion depth of colorectal cancer (CRC) located on a colorectal fold. Methods: The cases of 22 patients with 22 fold-located CRCs (12 early CRCs and 10 advanced CRCs) who underwent a pretherapeutic computed tomographic colonography were retrospectively examined. T1-stage CRCs were classified into two categories according to the Japanese guideline: T1a-stage, carcinoma invading the superficial submucosa (< 1000 um); and T1b-stage, carcinoma invading the deeper submucosa ([&ge;] 1000 um). The maximum width of a lateral contour of the colorectal fold on which the CRC was located, i.e., the gap distance between the two adjacent haustrations, was calculated from the lateral split-view CT enema image by two gastrointestinal radiologists. These values were compared between the intramucosal / T1a CRCs and the T1b / more deeply invading CRCs. The inter-rater intraclass correlation coefficients were also evaluated for reliability. Results: The maximum widths of a lateral contour of the colorectal fold were significantly higher in the T1b / more deeply invading CRCs than in the intramucosal / T1a CRCs (p<0.0001). The optimum cut-off value of the maximum width of a lateral contour of the colorectal fold for differentiating the former from the latter was 6.0 mm, with a sensitivity and specificity of 100% and 100%, respectively. The inter-rater intraclass correlation coefficient for measurement of a lateral contour of the colorectal fold was 0.958. Conclusions: We demonstrated for the first time that quantitative evaluation of a width of lateral contour using a lateral split-view CT enema image can improve the diagnostic accuracy of the invasion depth for CRCs located on a colorectal fold.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Interpreting Health Differences between Self-reported Black and White Children in U.S.: Insights from a Methodological Perspective\nAbstract: Understanding health differences among racial groups in child development is crucial for addressing inequalities that may affect various aspects of a child's life. However, factors such as household and neighborhood socioeconomic status (SES) often covary with health differences between races, making it challenging to accurately reveal these differences using conventional covariate-control methods such as multiple regression. Alternative methods, such as Propensity Score Matching (PSM), may provide better covariate control. Supporting this notion, we found that PSM is more sensitive than regression-based methods in detecting health differences between self-reported Black and White children across a wide range of behavioral and neural measurements in the ABCD (5636 White, 1350 Black). Puberty status, an index of physical maturation, emerged as the largest difference between races and mediated the health differences between races on the majority of behavioral and neural variables. These findings highlight the importance of controlling for pubertal status and using more effective covariate-control methods to accurately represent health differences between Black and White children.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Quantifying the Trainability of Peripheral Nerve Function in Young and Older Adults\nAbstract: It is well known that the natural progression of age can result in motor neuron degeneration. Consequently, this leads to slowing of nerve conduction, denervation, and reduced motor function. Slower neural conduction can negatively alter an individual's response time, which could increase the risk of falls. Further investigation is needed to determine the potential role exercise interventions may afford in mitigating age-related nerve deterioration. The purpose was two-fold: first, to determine the effects of resistance training on nerve conduction velocity (NCV), and second, to determine if changes in NCV are dependent on age. We hypothesized that training would result in faster nerves in both young and older adults, albeit to a lesser extent in older adults. Forty-eight subjects (18-84 yrs) completed this study (n = 26 younger, 22 older). Motor NCV and maximal strength were recorded before and after 4-weeks of handgrip training in both limbs. Training was conducted 3x/week with the use of a grip training kit. Mixed-factorial ANOVAs revealed significant increases in NCV for both the young (p < 0.001) and older training groups (p < 0.001), but neither control groups (p > 0.05). The young training group showed increased handgrip strength after four weeks (p = 0.004), while the other groups did not. The results of this study suggest that resistance training may be a viable method to counteract age-related nerve deterioration. These outcomes have the potential to improve quality of life and generate greater independence for our older populations.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Differential benefit of adjuvant everolimus according to endocrine therapy backbone in the randomized UNIRAD trial\nAbstract: Background: The randomized, double-blind UNIRAD trial evaluating the addition of 2 years of everolimus to endocrine therapy in patients with high-risk, early luminal breast cancer failed to demonstrate a benefit. We report the subgroup analyses. Patients and Methods: We randomized 1278 patients in a 1:1 ratio to receive 2 years of placebo or everolimus, added to endocrine therapy for up to 4 years after initiation. Randomization was stratified by endocrine therapy agent, prior adjuvant versus neoadjuvant therapy, progesterone receptor expression, and lymph node involvement. Subgroup analyses by each stratification factor were prespecified. Post hoc analyses were performed according to menopausal status and age. We also analyzed treatment adherence. Results: We observed a limited trend toward more favorable prognostic features in tamoxifen-treated patients, with more frequent ER+/PR+ tumors (88.5% vs. 84.1%, p=0.026) and less frequent pN2+ status (39.8% vs. 46%, p=0.032). In premenopausal women, we observed a numerical benefit of everolimus: 3y-DFS was 86% in the placebo group and 90% in the everolimus group [HR=0.76 (95%CI: 0.43-1.34)]. In premenopausal patients treated with tamoxifen (n=153; 12.3%), we observed an even stronger trend in favor of everolimus as 3-year DFS was 84% in the placebo group and 91% in the everolimus group [HR=0.54 (95%CI: 0.28-1.02)]. Early discontinuation of either everolimus or placebo was less frequent in the tamoxifen group than in the AI group: 48.0% vs. 56.9% (p=0.028). Conclusions: The present post-hoc analyses generate hypotheses regarding the interaction between menopausal status, tamoxifen and everolimus in patients with high-risk, ER-positive, HER2-negative early breast cancer. They suggest that tamoxifen alone is no longer the standard of care in high-risk premenopausal patients.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: The expression alteration of Golim4 (Gpp130) and its prognostic value in epithelial ovarian cancer\nAbstract: Background: Among common cancers in women, ovarian cancer has the lowest incidence; however, mortality rates are more than other gynecological cancers due to the lack of specific symptoms during early stages and absence of diagnostic markers. Golgi integral membrane protein 4 (GOLIM4) is a member of transporter complex which is located to the cis/medial of Golgi, and its role in tumor is unknown. In this study we aimed to assess the expression of Golim4 (GPP130) in epithelial ovarian cancer (EOC) patients. Methods: The expression levels of Golim4 in 58 malignant EOC specimens and 11 benign and normal control tissues were determined by real-time RT-PCR. Clinicopathological characteristics were analyzed. Non-parametric survival analysis was estimated by using Kaplan-Meier and log-rank test. Parametric survival analysis was evaluated through multivariate Cox regression analysis and generalized Gamma survival model. Result: Our findings showed that Golim4 was significantly down-regulated in Serous Papillary type of EOC tissue specimens than in non-cancerous tissues (P<0.001); however; the expression of Golim4 in total EOC tissue specimens revealed non-significant up-regulation. Kaplan- Meier analysis and log-rank test have suggested that EOC patients with low Golim4 expression have shorter overall survival when compared with patients with other expression groups (log-rank test P<0.05). Semi-parametric Cox proportional hazards model indicated that the status of Golim4 were independent predictor of overall survival in patients with EOC. Correlation analysis has been done by Spearman Rank correlation coefficient.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Probabilistic Graphical Models for Evaluating the Utility of Data-Driven ICD Code Categories in Pediatric Sepsis\nAbstract: Electronic health records (EHRs) are digitalized medical charts and the standard method of clinical data collection. They have emerged as valuable sources of data for outcomes research, offering vast repositories of patient information for analysis. Definitions for pediatric sepsis diagnosis are ambiguous, resulting in delayed diagnosis and treatment, highlighting the need for precise and efficient patient categorizing techniques. Nevertheless, the use of EHRs in research poses challenges. EHRs, although originally created to document patient encounters, are now primarily used to satisfy billing requirements. As a result, EHR data may lack granularity, potentially leading to misclassification and incomplete representation of patient conditions. We compared data-driven ICD code categories to chart review using probabilistic graphical models (PGMs) due to their ability to handle uncertainty and incorporate prior knowledge. Overall, this paper demonstrates the potential of using PGMs to address these challenges and improve the analysis of ICD codes for sepsis outcomes research.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: A novel modelling framework to simulate the effects of HIV stigma on HIV transmission dynamics\nAbstract: Introduction HIV remains a global public health challenge, with social determinants such as stigma influencing transmission dynamics, access to testing, and treatment. HIV stigma shapes both individual behaviour and community responses to HIV. However, modelling approaches have rarely represented the complex role of stigma in HIV epidemics. Our study introduces an innovative modelling framework designed to capture the interplay between stigma and HIV transmission dynamics. Methods We modelled effects of anticipated, internalised, and experienced HIV stigma on HIV testing, antiretroviral treatment initiation, and treatment adherence. We built an individual-based model representing the HIV epidemic (HIV-IBM) in a USA-like population of 3 million individuals that accounts for community demography, behaviour, and healthcare access. Stigma parameters were based on a scoping review focused on the prevalence and effects of stigma in people living with and without HIV. HIV-IBM was used to assess effects of interventions targeting different types of stigma. We tested reductions of stigma by 50% and 100% across the simulated population and performed a sensitivity analysis to identify effects of each type of stigma on the simulated HIV epidemic. Results Without reduced stigma, the HIV-IBM had an annual incidence rate of 12.6 (95% credible interval [CI]: 11.4-13.5) new cases per 100,000 people. Reducing the overall level of stigma in the population by 50% resulted in an annual incidence rate of 9.6 (95% CI: 8.6-10.3) per 100,000, and a 100% reduction in stigma resulted in an annual incidence rate of just 6.8 (95% CI: 6.1-7.3) per 100,000. In addition to reducing HIV incidence, reducing stigma resulted in a substantial increase of viral suppression among people living with HIV (50% stigma reduction: +10.5%; 100% stigma reduction: +16.4%). Sensitivity analysis showed that outcomes resulting from interventions targeting each type of stigma were highly heterogeneous. Conclusion Simulation results suggest that reducing HIV stigma could have a large effect on HIV incidence and viral suppression. Our model framework provides a dynamic approach to understanding the role of stigma in HIV outcomes that facilitates exploration of stigma reduction strategies and offers insights to inform evidence-based policies and interventions for reducing stigma and curtailing HIV.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Typical indicators of neighborhood change that are used to define gentrification have opposing associations with infant mortality\nAbstract: Infant mortality (IM), or death prior to the first birthday, is a key public health metric that increases with neighborhood structural inequities. However, neighborhood exposures shift as communities undergo gentrification, a pattern of neighborhood change defined by increasing affluence (in wealth, education, and housing costs). Gentrification has inconsistent associations with infant health outcomes like IM, which may be due to differing relationships between its composite measures and such outcomes. We designed a retrospective cohort analysis of all births and deaths from 2010-2019 across 4 metropolitan areas in Michigan to determine how gentrification and its neighborhood-change components are associated with risk of IM, using multilevel multivariable logistic regression models. Among 672,432 infants, 0.52% died before 1 year. IM was not associated with gentrification. Census tracts with greater increases in income and education had lower rates of IM, but tracts with greater increases in rent costs had higher rates of IM. In unadjusted models, odds of IM were 40% and 15% lower for infants living in tracts in the top quartile increase in household income and college completion, respectively, compared to infants from tracts with the least amount of change. Odds of IM were also increased 29% in infants from tracts with the most increases in rent, though these differences were attenuated when adjusting for individual social factors. Indicators of increasing community affluence have opposing relationships with IM. Policies and interventions that address rising housing costs may reduce IM.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Human bioaccumulation of micro- and nanoplastics is primarily determined by the organs' vascular volume\nAbstract: Recent studies have demonstrated the accumulation of micro- and nanoplastics (MNPs) in human organs, particularly in the brain, at an alarming rate (Campen et al., 2024, doi.org/10.21203/rs.3.rs-4345687/v1). This study examines the phenomenon of bioaccumulation in relation to up to six distinct biological and biomedical parameters pertaining to vascular structure and composition in three organ tissues: the brain cortex, kidney, and liver. As anticipated based on fundamental mechanistic considerations of bioaccumulation (washing rate), the MNP content appears to be proportional to the inverse of the capillary volume fraction ($V_c$), exhibiting no correlation with any other geometric variable, such as capillary diameter or capillary segment length. This finding is consistent with the small size spectra of MNPs in comparison to these dimensions. However, contrary to expectations, no correlation was observed between MNP content and biological composition (lipid, water, or carbohydrate content), while a likely non-causal correlation was observed between MNP content and protein content, which may be simply attributed to the non-linear correlation between protein content and $V_c$. The conclusions that point to a possibly exceedingly large number of MNP particles currently in our brains call for urgent further investigation and the revision of the published data.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Large language models for abstract screening in systematic- and scoping reviews: A diagnostic test accuracy study\nAbstract: Introduction We investigated if large language models (LLMs) can be used for abstract screening in systematic- and scoping reviews. Methods Two broad reviews were designed: a systematic review structured according to the PRISMA guideline with abstract inclusion based on PICO criteria; and a scoping review, where we defined abstract characteristics and features of interest to look for. For both reviews 500 abstracts were sampled. Two readers independently screened abstracts with disagreements handled with arbitrations or consensus, which served as the reference standard. The abstracts were analysed by six LLMs (GPT-4o, GPT-4T, GPT-3.5, Claude3-Opus, Claude3-Sonnet, and Claude3-Haiku). Primary outcomes were diagnostic test accuracy measures for abstract inclusion, abstract characterisation and feature of interest detection. Secondary outcome was the degree of automation using LLMs as a function of the error rate. Results In the systematic review 12 studies were marked as include by the human consensus. GPT-4o, GPT-4T, and Claude3-Opus achieved the highest accuracies (97% to 98%) comparable to the human readers (96% and 98%), although sensitivity was low (33% to 50%). In the scoping review 130 features of interest were present and the LLMs achieved sensitivities between 74-84%, comparable to the human readers (73% and 86%). The specificity of GPT-4o (98%) and GPT-4T (>99%) greatly surpassed the other LLMs (between 33% and 93%). For abstract characterization all LLMs achieved above 95% accuracy for language, manuscript type and study participant characterisation. For characterisation of disease-specific features only GPT-4T and GPT-4o showed very high accuracy. For abstract inclusion the highest automation rate (91%) at the lowest error rate (8%) was achieved by use of two LLMs with disagreement solved by human arbitration. An LLM pre screening before human abstract screening achieved an automation rate of 55% with no missed abstracts. Conclusion Abstract characterisation and specific feature of interest detection with LLMs is feasible and accurate with GPT-4o and GPT-4T. The majority of abstract screenings for systematic reviews can be automated with use of LLMs, at low error rates.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: A Method For Mimicking Tumour Tissue In Brain Ex-Vivo Ultrasound For Research Application And Clinical Training\nAbstract: Introduction: Intraoperative ultrasound is becoming a common tool in neurosurgery. However, effective simulation methods are limited. Current, commercial, and homemade phantoms lack replication of anatomical correctness and texture complexity of brain and tumour tissue in ultrasound images. Materials and Methods: We utilise ex-vivo brain tissue, as opposed to synthetic materials, to achieve realistic echogenic complexity and anatomical correctness. Agar, at 10-20\\% concentrate, is injected into the brain tissue to simulate the tumour mass. A commercially available phantom was purchased for benchmarking. Results: Qualitative analysis is performed by experienced professionals, measuring the impact of the addition of agar and comparing it to the commercial phantom. Overall, the use of ex vivo tissue was deemed more accurate and representative, compared to the synthetic materials-based phantom, as it provided good visualisation of real brain anatomy and good contrast within tissue. The agar tumour correctly produced a region of higher echogenicity with slight diffusion along the margin and expected interaction with the neighbouring anatomy. Discussion: Using human samples for training is limited due to high logistical and ethical challenges. Sparsity of online neurosurgical US data further compounds this. The proposed method successfully produced a better simulation of tumours in brain tissue than the phantom which suffers from surface texture dissimilarity, ultrasound echogenic homogeneity, and lack of anatomical correctness. Conclusion: The proposed method for creating tumour-mimicking tissue in brain tissue is inexpensive, accurate, and simple. Beneficial for both the trainee clinician and the researcher. A total of 576 annotated images are made publicly available upon request. Word count {approx} 3400.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Percentile Scores for the Revised University of Pennsylvania Smell Identification Test for 16,972 Individuals 60 years of Age and Older\nAbstract: Background and Objectives The University of Pennsylvania Smell Identification Test (UPSIT) is a widely employed objective measure of smell function. A revised version of this test, termed the UPSIT-R, was developed in 2020 to update certain odorants and response options. The goal of this study was to establish normative age- and sex-specific data for the UPSIT-R in a large adult cohort and to compare the resultant percentiles to those of the original UPSIT. A second study was performed to compare the relative performance of these two tests in a small cohort of persons with and without Parkinson's disease (PD). Methods UPSIT-R percentiles were derived separately by age (60-64, 65-69, 70-74, 75-59, and [&ge;]80 years) and sex (male, female) categories in 16,972 volunteers who were predominantly white and non-Hispanic. Percentiles were derived statistically using an empirical distribution averaging procedure. A Bland-Altman plot was employed to visualize the level of agreement between percentiles for the UPSIT-R and UPSIT. For the second study, non-parametric statistics were employed to compare the results of those with and without PD. Results UPSIT-R performance declined with increasing age; deficits were more pronounced in men than in women. The magnitude of the difference between the original and revised test percentile scores differed by age and sex; women had up to a 3-point improvement with the UPSIT-R whereas the oldest male age had up to a 3-point decline in percentile scores. Olfactory deficits in PD were confirmed on the UPSIT-R. Discussion This study provides normative data clinically useful for assessing the relative degree of dysfunction in persons 60 years of age and older using the UPSIT-R. Like the traditional version of the UPSIT, this version demonstrates that smell function declines with age and is generally better in women than in men, providing a solid clinical basis for physicians to accurately determine the degree of smell dysfunction in their patients.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Alternative per-protocol estimates: secondary analyses of data from the Balanced randomised controlled trial\nAbstract: Background The Balanced trial was designed to answer the question of whether anaesthetic depth affects postoperative mortality when a vulnerable patient undergoes major surgery. Patients were recruited between 2012 and 2017 at 73 centres in seven countries. In the intention-to-treat analysis (n=6644), there was no significant difference in one year mortality between patients randomised to surgery under deep (target BIS 35) or light anaesthesia (target BIS 50). However the separation between randomised groups was only 8.4 BIS units and the trial was criticised for being underpowered. Methods In a secondary analysis of this trial's data, we made alternative per-protocol estimates designed to improve the power of the trial. We added an additional covariate - each patient's deviation from target BIS - to the original analysis, statistically recreating the desired separation of 15 BIS units between randomised groups. We used multiple imputation to recover missing BIS values. We also assessed whether a proportional hazards Cox model was appropriate for the analysis of one year mortality. Results Our alternative per-protocol estimates did not differ materially from the original per-protocol estimate. The gain in precision through using all intent-to-treat patients for our per-protocol estimates was offset by the additional variance introduced when modelling missing BIS values. When modelling missing BIS, we found regional differences: in China, the separation between randomised groups was far higher (13.6 BIS units) than in any other region. Estimates and plots assessing proportional hazards suggested increasing late mortality under deep anaesthesia, most notably in China. Conclusion Our hypothesis is that deep anaesthesia in the Balance trial led to higher postoperative delirium, which in turn led to an increase in late mortality. In future trials, patients should be followed for more than a year and cause of death recorded.\n"}, {"role": "assistant", "content": "no"}]}
{"messages": [{"role": "system", "content": "You are a researcher that, given the title and the abstract of a pre print, you need to classify whether this pre print is about infectious disease or not (answer in yes or no)"}, {"role": "user", "content": "Title: Atherosclerotic fibrous plaques in females are characterized by endothelial-to-mesenchymal transition and linked to smoking\nAbstract: Background: Sex and plaque histology are intertwined, with fibrous atherosclerotic plaques being more prevalent in women and pointing to general smooth muscle cell plasticity and estrogen signaling. Plaque erosion, a significant contributor to acute coronary syndromes (ACSs), is linked to fibrous plaques and is more prevalent in women as compared to men. We hypothesize that the molecular drivers of histologically determined fibrous plaques differ between men and women. Methods: Human end-stage atherosclerotic plaques were isolated from consecutive patients who underwent carotid endarterectomy and were included in the Athero-Express biobank. Fibrous plaques from both female and male patients were histologically assessed and further processed to obtain protein, bulk RNA, single-cell RNA and DNA methylation data. We leveraged sex-differential expression and deconvolution analyses to uncover sex-biased molecular mechanisms and cellular dynamics which were experimentally validated using an EndMT in vitro model. Results: Out of 1,889 atherosclerotic plaques (1,309 male and 580 female), fibrous lesions were observed in 50% of female (n=290) and 31% of male patients (n=416). Compared to patients with atheromatous plaques (n=494), women with fibrous plaques exhibited a higher prevalence of smoking (41% vs. 33%), while men with fibrous plaques presented more often with diabetes (29% vs. 20%). Transcriptional and proteomic phenotyping highlighted more immune-dependent and inflammatory processes in male fibrous plaques. Genes and proteins with higher abundance in female fibrous plaques pointed to endothelial-to-mesenchymal transition (EndMT) and extracellular matrix remodelling. Using single-cell RNA sequencing, we identified a dominant role of endothelial and smooth muscle cells in female plaques, and more macrophages in males. Finally, at the cellular level, we show that sex -specific, smoking-mediated promoter methylation changes may explain these differences. Conclusions: Patients with end-stage fibrous atherosclerotic plaques have a distinct clinical profile, with men more often having diabetes and women more often smoking. This clinical profile associates with sex differences that point to different cellular and molecular compositions of fibrous plaques. These mechanisms might be candidate pathways to understand plaque erosion from a molecular point of view and may provide promising targets for atherosclerosis therapies, as they account for the sex-specific differences in plaque phenotype.\n"}, {"role": "assistant", "content": "no"}]}
